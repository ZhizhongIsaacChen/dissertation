\chapter{The VC-dimension of SQL queries and selectivity estimation through sampling}\label{ch:vcfreq}
\chaptermark{SQL queries and selectivity estimation}

\todo{Move the following to the appropriate place.

In our setting, for a class of select queries $Q$ on a table $\Tab$, $X$ is the
set of all tuples in the input table, and $R$ the family of the outputs (as sets
of tuples) of the queries in $Q$ when run on 
%$X$, i.e. on 
$\Tab$. For a class
$Q$ of queries combining select and join operations, $X$ is the Cartesian
product of the associated tables and $R$ is the family of outcomes of queries in
$Q$, seen as $\ell$-uples of tuples, if $\ell$ tables are involved in the
queries of $Q$.  When the context is clear we identify the family $R$ with a
class of queries.

When the ranges represent all the possible outputs of queries in a class $Q$
applied to database tables $\Db $, the VC-dimension of the range space is the maximum
number of tuples such that any subset of them is the output of a query in $Q$.

In Sect.~\ref{sec:vcfreqapplications} we use an
$\varepsilon$-approximation (or a relative $(p,\varepsilon)$-approximation)  to
compute good estimates of the selectivities of all queries in $Q$. We obtain a
small approximation set through probabilistic construction.
The challenge in applying Thm.~\ref{thm:eapprox} to our setting is computing the
VC-dimension of a range space defined by a class of queries. 

}

\section{Introduction}\label{sec:vcfreqintro}
As advances in technology allow for the collection and storage of vast
databases, there is a growing need for \emph{advanced machine learning
techniques} for speeding up the execution of queries on such large datasets. In this work we
focus on the fundamental task of estimating the selectivity, or output size, of
a database query, which is a crucial step in a number of query processing tasks
such as execution plan optimization and resource allocation in parallel and
distributed databases. The task of efficiently obtaining such accurate estimates
has been extensively studied in previous work with solutions ranging from
storage of pre-computed statistics on the distribution of values in the tables,
to online sampling of the databases, and to combinations of the two
approaches~\citep{LiptonN95,LiptonNS90,HaasS92,HouOD91,HaasS95,GangulyGMS96,GantiLR00,GibbonsM98,HouOT88,LarsonLZZ07,PoosalaI97}.
Histograms, simple yet powerful statistics of the data in the tables, are the most
commonly used solution in practice, thanks to their computational and space
efficiency. However, there is an inherent limitation to the accuracy of this
approach when estimating the selectivity of queries that involve either multiple
tables/columns or correlated data. Running the query on freshly sampled data
gives more accurate estimates at the cost of delaying the execution of the query
while collecting random samples from a disk or other large storage medium and
then performing the analysis itself. This approach is therefore usually more
expensive than a histogram lookup. Our goal in this work is to exploit both the
computational efficiency of using pre-collected data and the provable accuracy
of estimates obtained by running a query on a properly sized random sample of
the database.

We apply the statistical concept of Vapnik-Chervonenkis (VC)
dimension~\citep{VapnikC71} to develop and analyze a novel technique to generate
accurate estimates of query selectivity. Roughly speaking, the VC-dimension of a
collection of indicator functions (hypotheses) is a measure of its complexity or
expressiveness. A major
theoretical contribution of this work, which is of independent interest, is an
explicit bound to the VC-dimension of various classes of queries, viewed as
indicator functions on the
Cartesian product of the database tables. In particular, we show an upper bound
to the 
VC-dimension of a class of queries that is a function of the maximum number of
Boolean, select and join operations in any query in the class, but it is not
a function of the number of different queries in the class. By adapting a
fundamental result from the VC-dimension theory to the database setting, we
develop a method that for any family of queries, defined by its VC-dimension,
builds a concise sample of the database, such that with high probability, the
execution of \emph{any} query in the class on the sample provides an accurate
estimate for the selectivity of the query on the original large database. The
error probability holds \emph{simultaneously} for the selectivity estimate of
\emph{all} queries in the collection, thus the same sample can be used to
evaluate the selectivity of multiple queries, and the sample needs to be
refreshed only following major changes in the database. The size of the sample
does not depend on the size (number of tuples) in the database, just on the
complexity of the class of queries we plan to run, measured by its VC-dimension.
Both the analysis and the experimental results show that accurate selectivity
estimates can be obtained using a sample of a surprisingly small size (see
Table~\ref{tab:samplesize} for concrete values), which can then reside in main
memory, with the net result of a significant speedup in the execution of
queries on the sample. 

A technical difficulty in applying the VC-dimension results to the database
setting is that they assume the availability of a uniform sample of the
Cartesian product of all the tables, while in practice it is more efficient to
store a sample of each table separately and run the queries on the Cartesian
product of the samples, which has a different distribution than a sample of the
Cartesian product of the tables. We develop an efficient  procedure for
constructing a sample that circumvents this problem (see
Sect.~\ref{sec:vcfreqapplications}).

We present extensive experimental results that validate our theoretical analysis
and demonstrate the advantage of our technique when compared to complex
selectivity estimation techniques used in PostgreSQL and the Microsoft SQL
Server. The main advantage of our method is that it gives provably accurate
predictions for the selectivities of all queries with up to a given complexity
(VC-dimension) specified by the user before creating the sample, while
techniques like multidimensional histograms or join synopses are accurate only
for the queries for which they are built.

Note that we are only concerned with estimating the selectivity of a query, not
with approximating the query answer using a sample of the database.
\citet{Das09} presents a survey of the possible solutions to this latter task. %
%In the future, we will investigate on the application of VC-dimension to
%approximate query processing.

\paragraph{Outline.} The rest of the paper is organized as follows. We review
the relevant previous work in Sect.~\ref{sec:vcfreqprevwork}. In
Sect.~\ref{sec:vcfreqprelim} we formulate the problem and introduce the
Vapnik-Chervonenkis dimension and the related tools we use in developing our
results. Our main analytical contribution, a bound on the VC dimension  of class of queries is presented in Sect.~\ref{sec:vcfreqvcdimqueries}.  The application of these results for selectivity estimation is
given in Sect.~\ref{sec:vcfreqapplications}. Experiments are presented in
Sect.~\ref{sec:vcfreqexperiments}. 

\section{Related Work}\label{sec:vcfreqprevwork}
Methods to estimate the selectivity (or cardinality of the output) of queries
have been extensively studied in the database literature primarily due to the
importance of this task  to query plan optimization and resource allocation. A
variety of approaches have been explored, ranging from the use of sampling, both
online and offline, to the pre-computation of different statistics such as
histograms, to the application of methods from machine
learning~\citep{ChenMM90,HarangsriNS97}, data mining~\citep{GryzL04},
optimization~\citep{ChaudhuriDN07,MarklHKMST07}, and probabilistic
modeling~\citep{GetoorTK01,ReS10}.

The use of sampling for selectivity estimation has been studied mainly in the
context of online sampling~\citep{LiptonNS90,LiptonN95}, where a sample is
obtained, one tuple at a time, after the arrival of a query and it used only
to evaluate the selectivity of that query and then discarded. Sampling at random
from a large database residing
on disk is an expensive operation~\citep{Olken93,BrownH06,GemullaLH06}, and in
some cases sampling for an accurate cardinality estimate is not significantly
faster than full execution of the query~\citep{HaasNSS93,HaasNS94}.

A variety of sampling and statistical analysis techniques has been tested to 
improve the efficiency of the sampling procedures and in particular
to identify early stopping conditions. These include sequential sampling
analysis~\citep{HouOD91,HaasS92}, keeping additional statistics to improve the
estimation~\citep{HaasS95}, labelling the tuples and using label-dependent
estimation procedures~\citep{GangulyGMS96}, or applying the cumulative distribution
function inversion procedure~\citep{WuAE01}. Some works also looked at non-uniform
sampling~\citep{BabcockCD03,EstanN06} and stratified
sampling~\citep{ChaudhuriDN07,JoshiJ08}. Despite all these relevant
contributions, online sampling is still considered too expensive for most
applications. An off-line sampling approach was explored by~\citet{NguHS04}, who
used systematic sampling (requiring the
tuples in a
table to be sorted according to one of the attributes) with a
sample size dependent on the number of tuples in the table. Their work does
not give any explicit guarantee on the accuracy of the predictions.
~\citet{ChaudhuriDN07} present an approach which uses
optimization techniques to identify suitable strata before sampling. The
obtained sample is such that the mean square error in estimating the selectivity
of queries belonging to a given workload is minimized, but there is no quality
guarantee on the maximum error. \citet{Haas96} developed Hoeffding
inequalities to bound the probability that the selectivity of a query estimated
from a sample deviates more than a given amount from its expectation.
However, to estimate the selectivity for multiple queries and obtain a given
level accuracy for all of them, simultaneous statistical inference techniques
like the union bound should be used, which are known to be overly conservative
when the number of queries is large~\citep{Miller81}. In contrast, our result
holds simultaneously for \emph{all} queries within a given complexity (VC-dimension). 

A technical problem arises when combining join operations and sampling. As
pointed out by \citet{ChaudhuriMN99}, the Cartesian
product of uniform samples of a number of tables is different from a uniform sample of the
Cartesian product of those tables. Furthermore, given a size $s$, it is
impossible to a priori determine two sample sizes $s_1$ and $s_2$ such that
uniform samples of these sizes from the two tables will give, when joined
together along a common column, a sample of the join table of size $s$. In
Sect.~\ref{sec:vcfreqapplications} we explain why only the first issue is of concern
for us and how we circumvent it.

In practice most database systems use pre-computed statistics to predict query
selectivity~\citep{HouOT88,GibbonsM98,GantiLR00,JinGJA06,LarsonLZZ07}, with
histograms being the most commonly used representation. The construction,
maintenance, and use of histograms were thoroughly examined in the
literature~\citep{JagadishKMPSS98,IoannidisP95,MatiasVW98,PoosalaHIS96}, with
both theoretical and experimental results. In particular~\citet{ChaudhuriMN98}
rigorously evaluated the size of the
sample needed for
building a histogram providing good estimates for the selectivities of a large
group of (select only, in their case) queries.  \citet{KaushikNRC05} extensively compared histograms and
sampling from a
space complexity point of view, although their sample-based estimator did not
offer a uniform probabilistic guarantee over a set of queries and they only
consider the case of foreign-key equijoins. We address both these points in our
work. Although very efficient in terms of storage needs and query time, the
quality of estimates through histograms is inherently limited for complex
queries because of two major drawbacks in the use of histograms: intra-bucket
uniformity assumption (i.e., assuming a uniform distribution for the frequencies
of values in the same bucket) and inter-column independence assumption (i.e.,
assuming no correlation between the values in different columns of the same or
of different tables).  Different authors suggested solutions to improve the
estimation of selectivity without making the above
assumptions~\citep{BrunoC04,Dobra05,PoosalaI97,WangVI97,WangS03}. Among these
solutions, the use of multidimensional
histograms~\citep{BrunoCG01,PoosalaI97,SrivastavaHMKT,WangS03} seems the most
practical. Nevertheless, these techniques are not widespread due to the extra
memory and computational costs in their implementation.

Efficient and practical techniques for drawing random samples from a database
and for updating  the sample when the underlying tables evolve have been extensively
analyzed in the database
literature~\citep{BrownH06,GemullaLH06,GemullaLH07,HaasK04,JermainePA04}.

The {\em Vapnik-Chervonenkis dimension} was first introduced in a seminal
article~\citep{VapnikC71} on the convergence of sample averages to their
expectations, but it was only with the work of~\citet{HausslerW86}
and~\citet{BlumerEHW89} that it was applied to learning. Since then,
VC-dimension has encountered enormous success and application in the fields of
computational geometry~\citep{Chazelle00,Matousek02} and machine
learning~\citep{AnthonyB99,MohriRT12} but
its use in system-related fields is not as widespread. In the database
literature, it was used in the context of constraint databases to compute good
approximations of aggregate operators~\citep{BenediktL02}. VC-dimension-related
results were also recently applied in the field of database privacy
by~\citet{BlumLR08} to show a bound on the number of queries needed
for an attacker to learn a private concept in a database. \citet{Gross11} showed
that content with unbounded VC-dimension can not be watermarked for privacy
purposes. \citet{RiondatoU12} used VC-dimension to compute a sample size
sufficient for computing high-quality approximations of the collections of
Frequent Itemsets and Association Rules in a transactional dataset.

To the best of our knowledge, our work is the first to provide explicit bounds
on the VC-dimension of SQL queries and to apply the results to query selectivity
estimation.

\section{Preliminaries}\label{sec:vcfreqprelim}
In this Section, we introduce the necessary terminology, concepts, and tools
that we will use to develop our results in the following Sections.

\subsection{Database Queries and Selectivity}
We outline here some basic definitions about databases, queries, and
selectivity. We refer the reader to complete textbooks for additional
information~\citep{GarciaMolinaUW02}.
We consider a \emph{database} $\Db$ of $k$ tables $\Tab_1,\dotsc,\Tab_k$. 
A \emph{table} is a two-dimensional representation of data. Its rows are called
\emph{tuples} and it can have multiple \emph{columns}.  We denote a
column $C$ of a table $\Tab$ as $\Tab.C$ and, for a tuple $t\in\Tab$, the value
of $t$ in the column $C$ as $t.C$. We denote the domain of the values that can
appear in a column $\Tab.C$ as $D(\Tab.C)$. Table~\ref{tab:example} shows two
examples of database tables, \texttt{Customers} and \texttt{CarColors}. Our
focus is on queries that combine select and join operations, defined as follows.
We do not take projection operations into consideration because their
selectivities have no impact on query optimization.

\begin{table}[htb]
  \centering
  \begin{tabular}{c|c|c|c}
    %\toprule
    \multicolumn{4}{c}{\emph{Customers}} \\
    \midrule
    \emph{Name} & \emph{Street} & \emph{ZipCode} & \emph{PhoneNumber} \\
    \midrule
    \texttt{John Doe} & \texttt{Pratt Av.} & \texttt{02906} & \texttt{401-1234567} \\
    \texttt{Jim Clark} & \texttt{Morris Rd.} & \texttt{02906} & \texttt{502-8902134} \\
    \texttt{Greta Garbo} & \texttt{Pitman St.} & \texttt{05902} & \texttt{853-9876543} \\
    \bottomrule
  \end{tabular}
  \hfill
  \begin{tabular}{c|c}
    \multicolumn{2}{c}{\emph{CarColor}} \\
    \midrule
    \emph{Name} & \emph{Color} \\
    \midrule
    \texttt{John Doe} & \texttt{Blue} \\
    \texttt{Jane Doe} & \texttt{Red} \\
    \texttt{Greta Garbo} & \texttt{Red}\\ 
    \bottomrule
  \end{tabular}
  \caption{Example of database tables.}
  \label{tab:example}
\end{table}

\begin{definition}\label{def:selectquery}
  Given a table $\Tab$ with columns $\Tab.C_1,\dotsc,\Tab.C_\ell$, a
  \emph{selection query} $q$ on $\Tab$ is an operation which returns a subset
  $S$ of the tuples of $\Tab$ such that a tuple $t$ of $\Tab$ belongs to $S$ if
  and only if the values in $t$ satisfy a condition $\mathcal{C}$
  (the \emph{selection predicate}) expressed by $q$. In full
  generality, $\mathcal{C}$ is the Boolean combination of clauses of the form
  $\Tab.C_i \op a_i$, where $\Tab.C_i$ is a column of $\Tab$, ``$\op$'' is one
  of $\{<,>,\ge,\le,=,\neq\}$ and $a_i$ is an element of the domain of
  $\Tab.C_i$.
\end{definition}

As an example, the selection query 
\[
\mbox{\texttt{SELECT} } * \mbox{ \texttt{FROM} \emph{Customers},  \texttt{WHERE}
\emph{Customers.ZipCode}} = 02906
\]
would return the first and second row of Table~\ref{tab:example}.

We assume that all $D(\Tab.C_i)$ are such that it is possible to build total
order relations on them. This assumptions does not exclude categorical domains
from our discussion, because the only meaningful values for ``$\op$'' for such
domains are ``$=$'' and ``$\neq$'', so we can just assume an arbitrarily but fixed
order for the categories in the domain.

\begin{definition}\label{def:joinquery}
  Given two tables $\Tab_1$ and $\Tab_2$, a $\emph{join query}$ $q$ on a common
  column $C$ (i.e., a column present both in $\Tab_1$ and $\Tab_2$) is an
  operation which returns a subset of the Cartesian product of the tuples in
  $\Tab_1$ and $\Tab_2$. The returned subset is defined as the set
  \[
  \{(t_1,t_2) ~:~ t_1\in\Tab_1, t_2\in\Tab_2, \mbox{ s.t. } t_1.C \op t_2.C \}\]
  where ``$\op$'' is one of $\{<,>,\ge,\le,=,\neq\}$.
\end{definition}

An example of a join query is the following:
\[
\mbox{\texttt{SELECT} } * \mbox{ \texttt{FROM} \emph{Customers}, \emph{CarColors} \texttt{WHERE}
\emph{Customers.Name}} = \mbox{\emph{CarColors.Name}}
\]
This query would return the following tuples:
\begin{align*}
  &(\mbox{John Doe}, \mbox{Pratt Av.}, \mbox{02906}, \mbox{401-1234567}, \mbox{Blue}), \\
  &(\mbox{Greta Garbo}, \mbox{Pitman St.}, \mbox{05902}, \mbox{853-9876543}, \mbox{Red})
\end{align*}
The column \emph{Name} is reported only once for clarity.

Our definition of a join query is basically equivalent to that of a
\emph{theta-join}~\citep[Sect.5.2.7]{GarciaMolinaUW02}, with the limitation that
the join condition $\mathcal{C}$ can only contain a single clause, i.e., a single
condition on the relationship of the values in the shared column $C$ and only
involve the operators $\{<,>,\ge,\le,=,\neq\}$ (with their meaning on $D(C)$).
The pairs of tuples composing the output of the join in our definition have a
one-to-one correspondence with the tuples in the output of the corresponding
theta-join.

\begin{definition}\label{def:general query}
  Given a set of $\ell$ tables $\Tab_1,\dotsc,\Tab_\ell$, a
  \emph{combination of select and join operations} is a query returning a subset
  of the Cartesian product of the tuples in the sets $S_1,\dotsc,S_\ell$, where
  $S_i$ is the output of a selection query on $\Tab_i$. The returned set is
  defined by the selection queries and by a set of join queries on
  $S_1,\dotsc,S_\ell$.
\end{definition}
As an example, the query 
\begin{align*}
\mbox{\texttt{SELECT} } &* \mbox{ \texttt{FROM} \emph{Customers},
\emph{CarColors} \texttt{WHERE} 
\emph{Customers.Name}} = \mbox{\emph{CarColors.Name}} \\
& \mbox{\texttt{AND} \emph{Customers.ZipCode}}=02906 \mbox{ \texttt{AND}
\emph{CarColors.Color}}=\mbox{\emph{Red}}
\end{align*}
combines select and joins operations. It returns no tuple (empty answer), as
there is no individual reported in both tables with zipcode $02906$ and a red
car.

\begin{definition}\label{def:exectree}
  Given a query $q$, a \emph{query plan} for $q$ is a directed binary tree
  $T_q$ whose nodes are the elementary (i.e., select or join) operations into
  which $q$ can be decomposed. There is an edge from a node $a$ to a node $b$ if
  the output of $a$ is used as an input to $b$. The operations on the leaves of
  the tree use one or two tables of the database as input. The output of the
  operation in the root node of the tree is the output of the query.
\end{definition}

It follows from the definition of a combination of select and join operations
that a query may conform with multiple query plans. Nevertheless, for all the
queries we defined there is (at least) one query plan such that all select
operations are in the leaves and internal nodes are join
nodes~\citep{GarciaMolinaUW02}. To derive our results, we use these specific
query plans.

Two crucial definitions that we use throughout the work are the 
\emph{cardinality} of the output of a query and the equivalent concept of
\emph{selectivity} of a query.

\begin{definition}\label{def:cardsel}
  Given a query $q$ and a database $\Db$, the \emph{cardinality} of its output
  is the number of elements (tuples if $q$ is a selection query, pairs of
  tuples if $q$ is a join query, and $\ell$-uples of tuples for combinations of
  join and select involving $\ell$ tables) in its output, when run on $\Db$. The
  \emph{selectivity} $\sigma(q)$ of $q$ is the ratio between its cardinality and
  the product of the sizes of its input tables.
 \end{definition}

Our goal is to store a succinct representation (sample) $\Sam$ of
the database $\Db$ such that an execution of a query on the sample $\Sam$ will
provide an accurate estimate for the selectivity of
each operation in the query plan when executed on the database $\Db$. 

\section{The VC-dimension of Classes of Queries}\label{sec:vcfreqvcdimqueries}
In this section we develop a general bound to the VC-dimension of classes of
queries. We start by computing the VC-dimension of simple select queries on one
column and then move to more complex queries (multi-attributes select
queries, join queries). We then extend our bounds to general queries that are
combinations of multiple select and join operations.

\subsection{Select Queries}\label{sec:vcfreqvcdimselqueries}
Let $\Tab$ be a table with $m$ columns $\Tab.C_1,\dotsc,\Tab.C_m$, and $n$
tuples. For a fixed column $\Tab.C$, consider the set $\Sigma_C$ of the
selection queries in the form 
\begin{equation}\label{eq:vcfreqselect}
\mbox{\texttt{SELECT }} *\mbox{\texttt{ FROM }} \Tab \mbox{\texttt{ WHERE }}
\Tab.C_i \op a \end{equation}
where $\op$ is an \emph{inequality} operator (i.e., either ``$\ge$'' or
``$\le$'')\footnote{The operators
``$>$'' and ``$<$'' can be reduced to ``$\ge$'' and ``$\le$'' respectively.} and
$a\in D(\Tab.C)$. 

Let $q_1,q_2\in\Sigma_C$ be two queries. We say that $q_1$ is equivalent to
$q_2$ (and denote this fact as $q_1=q_2$) if their outputs are identical, i.e.,
they return the same set of tuples when they are run on the same database. Note
that $q_1=q_2$ defines a proper equivalence relation. 

Let $\Sigma^*_C \subseteq \Sigma_C$ be a maximum subset of $\Sigma_C$ that
contains no equivalent queries, i.e., it contains one query from each equivalent
class.
%
%the set of queries built as follows. Let
%$q_a,q_b\in\Sigma_C$ be two equivalent queries such that
%\begin{align*}
%  q_a &= \mbox{\texttt{ SELECT }} *\mbox{\texttt{ FROM }} \Tab \mbox{\texttt{ WHERE }} \Tab.C \op a\\
%  q_b & =\mbox{\texttt{ SELECT }} *\mbox{\texttt{ FROM }} \Tab \mbox{\texttt{ WHERE }} \Tab.C \op b
%\end{align*}
%and such that $a < b$.
%
%If $\op=``\le"$ (resp. $\op=``\ge"$), then we put $q_a$ (resp. $q_b$) in
%$\Sigma^*_C$. It is easy to see that for any pair of queries
%$q_1,q_2\in\Sigma^*_C$, we have $q_1\neq q_2$.

%What is the size of $\Sigma^*_C$ ? It is the number of different subsets of
%tuples from $\Tab$ we can obtain by asking queries from $\Sigma_C$. This quantity
%is equal to the number of distinct values from $D(\Tab_C)$ in the tuples of
%$\Tab$. More formally,
%\[
%|\Sigma^*_C|=|\{a\in D(\Tab.C) ~:~ \exists t\in\Tab\mbox{ s. t. } t.C=a\}|.\]
%The size of $\Sigma^*_C$ is maximized if all the tuples $t$ in $\Tab$ have a
%different value $t.C$, and we consider such a table $\Tab$ because a larger set
%of ranges may lead to a higher VC-dimension for $(\Tab,\Sigma^*_C)$.

\begin{lemma}\label{lem:vcdimselmulcol}
  Let $\Tab$ be a table with $m$ columns $C_i$, $1\le i\le m$, and consider the
  set of queries 
  \[\Sigma^*_\Tab=\bigcup_{i=1}^m \Sigma^*_{C_i},\]
  where
  $\Sigma^*_{C_i}$ is defined as in the previous paragraph. Then, the range
  space $S=(\Tab,\Sigma^*_\Tab)$ has VC-dimension at most $m+1$.
\end{lemma}

\begin{proof}
  We can view the tuples of $\Tab$ as points in the $m$-dimensional space
  $\Delta=D(\Tab.C_1)\times D(\Tab.C_2) \times \dotsc\times D(\Tab.C_m)$. A tuple
  $t\in\Tab$ such that $t.C_1=a_1, t.C_2=a_2, \ldots, t.C_m=a_m$ is represented
  on the space by the point $(a_1,a_2,\dotsc,a_m)$.

  The queries in $\Sigma^*_\Tab$ can be seen as half spaces of $\Delta$. In
  particular any query in $\Sigma^*_\Tab$ is defined as in~\eqref{eq:vcfreqselect}
  and can be seen as the half space 
  \[
  \{(x_1,\dotsc,x_i,\dotsc,x_m) ~:~ x_j\in D(\Tab_j)\mbox{ for } j\neq i, \mbox{
  and } x_i \op a_i\}\subseteq \Delta\enspace.\
  \]
  It then follows from Lemma~\ref{lem:matousek} that $\VC(S)\le m+1$.
\end{proof}

We now extend these result to general selection queries. Consider the set
$\Sigma^{2*}_\Tab$ of queries whose selection predicate can be expressed as the
Boolean combination of the selection predicates of at most two queries from
$\Sigma^*_\Tab$. These are the queries of the form:
\[
\mbox{\texttt{SELECT }} * \mbox{\texttt{ FROM }} \Tab \mbox{\texttt{ WHERE }}
\Tab.X_1 \op_1 a_1 \bool \Tab.X_2 \op_2 a_2
\]
where $\Tab.X_1$ and $\Tab.X_2$ are two columns from $\Tab$ (potentially,
$\Tab.X_1=\Tab.X_2$), $a_1\in D(\Tab.X_1)$, $a_2\in D(\Tab.X_2)$, ``$\op_i$'' is
either ``$\ge$'' or ``$\le$'' and ``$\bool$'' is either ``\texttt{AND}'' or
``\texttt{OR}''. Note that, in particular the queries in the form
\[
\mbox{\texttt{SELECT }} * \mbox{\texttt{ FROM }} \Tab \mbox{\texttt{ WHERE }} \Tab.X_1 \eqop a
\]
where $\eqop$ is either ``$=$'' or ``$\neq$'', belong to $\Sigma^{2*}_\Tab$
because we can rewrite a selection predicate containing one of these operators as
a selection predicate of two clauses using ``$\ge$'' and ``$\le$'' joined by
either $AND$ (in the case of ``$=$'') or $OR$ (in the case of ``$\neq$'').

By applying Lemma~\ref{lem:genboolcomp}, we have that the VC-dimension of the
range space $(\Tab,\Sigma^{2*}_\Tab)$ is at most $3(m+1)2\log((m+1)2)$, where
$m$ is the number of columns in the table $\Tab$.

We can generalize this result to $b$ Boolean combinations of selection
predicates as follows.

\begin{lemma}\label{lem:vcdimselgen}
  Let $\Tab$ be a table with $m$ columns, let $b>0$ and let $\Sigma^{b*}_\Tab$
  be the set of selection queries on $\Tab$ whose selection predicate is a
  Boolean combination of $b$ clauses. Then, the VC-dimension of the range space
  $S_b = (\Tab, \Sigma^{b*}_\Tab)$ is at most $3((m+1)b)\log((m+1)b)$.  
\end{lemma}

Note that we can not apply the bound
used in the proof of Lemma~\ref{lem:vcdimselmulcol} since
not all queries in $\Sigma^{b*}_\Tab$ are equivalent to axis-aligned
boxes. Once we apply Boolean operations on the outputs of the individual select
operation, the set of possible outputs, $S_b = (\Tab, \Sigma^{b*}_\Tab)$ , may
form complex subsets, including unions of disjoint (half-open) axis
aligned-rectangles and/or intersections of overlapping ones that cannot be
represented as a collection of half spaces. Thus, we need to apply a different technique here.

\begin{proof} 
 The output of a query $q$ in $\Sigma^{b^*}_\Tab$ can be seen as the Boolean
 combination (i.e., union and intersection) of the outputs of at most $b$
 "simple" select queries $q_i$ from $\Sigma^{*}_\Tab$ where each of these
 queries $q_i$ is as in~\eqref{eq:vcfreqselect}. An \texttt{AND} operation in the
 predicate of $q$ implies an intersection of the outputs of  the corresponding
 two queries $q_i$ and $q_j$, while an \texttt{OR} operation implies a union of
 the outputs. The thesis follows by applying Lemma~\ref{lem:genboolcomp}.
\end{proof}

\subsection{Join Queries}\label{sec:vcfreqvcdimjoinqueries}
Let $\Tab_1$ and $\Tab_2$ be two distinct tables, and let $R_1$ and $R_2$
be two families of (outputs of) select queries on the tuples of $\Tab_1$ and
$\Tab_2$ respectively. Let $S_1=(\Tab_1,R_1)$, $S_2=(\Tab_2,R_2)$ and let
$\VC(S_1),\VC(S_2)\geq 2$. Let $C$ be a column along which $\Tab_1$ and $\Tab_2$
are joined, and let $T_J=\Tab_1\times\Tab_2$ be the Cartesian product of the two
tables.

For a pair of queries $r_1\in R_1$, $r_2\in R_2$, let
\[
J^{\op}_{r_1,r_2}=\{(t_1,t_2) ~:~ t_1\in
r_1, t_2\in r_2, t_1.C \op t_2.C\},\]
where $\op\in\{>,<,\ge,\le,=,\neq\}$. $J^{\op}_{r_1,r_2}$ is the set of ordered pairs of
tuples (one from $\Tab_1$ and one from $\Tab_2$ that forms the output of the
join query 
\begin{equation}\label{eq:vcfreqjoinq}
\mbox{\texttt{SELECT }} *\mbox{\texttt{ FROM }} \Tab_1,\Tab_2 \mbox{\texttt{
WHERE }} r_1 \mbox{\texttt{ AND }} r_2 \mbox{\texttt{ AND }} \Tab_1.C \op
\Tab_2.C\enspace.
\end{equation}
Here we simplify the notation by identifying select queries with
their predicates. We have $J^{\op}_{r_1,r_2}\subseteq
r_1\times r_2$ and $J^{\op}_{r_1,r_2}\subseteq T_J$. Let 
\[ J_C =\{J^{\op}_{r_1,r_2}~|~r_1\in R_1, r_2\in R_2, \op\in\{>,<,\ge,\le,=,\neq\}
\}.\]
$J_C$ is the set of outputs of all join queries like the one in~\eqref{eq:vcfreqjoinq},
for all pairs of queries in $R_1\times R_2$ and all values of ``$\op$''. We
present here an upper bound to the VC-dimension of the range space
$S_J=(T_J,J_C)$.

\begin{lemma}\label{lem:vcdimjoin}
  $\VC(S_J)\leq 3(\VC(S_1)+\VC(S_2))\log ((\VC(S_1)+\VC(S_2))).$
\end{lemma}

\begin{proof}
  Let $v_1=\VC(S_1)$ and $v_2=\VC(S_2)$. Assume that a set $A\subseteq T_J$ is
  shattered by $J_C$, and $|A|=v$.  Consider the two cross-sections $A_1 =
  \{x\in \Tab_1 ~:~ (x,y)\in A\}$ and $A_2 = \{y\in \Tab_2 ~:~ (x,y)\in A\}$.
  Note that $|A_1|\leq v$ and $|A_2|\leq v$ and
  by~\citep[Corol.~14.4.3]{AlonS08}, %~\ref{corol:SauerProj}
  $|P_{R_1} (A_1)|\leq g(v_1,v)\le v^{v_1}$ and $|P_{R_2}
  (A_2)|\leq g(v_2,v) \le v^{v_2}$. For each set $r \in
  P_{J_C}(A)$ (i.e., for each subset $r\subseteq A$, given that
  $P_{J_C}(A)=2^A$)  there is a pair $(r_1,r_2)$, $r_1\in R_1$, $r_2\in R_2$,
  and there is $\op$, such that $r = A \cap J^{\op}_{r_1,r_2}$. Each of such
  pair $(r_1,r_2)$ identifies a distinct pair $(r_1\cap A_1, r_2\cap A_2) \in
  P_{R_1}(A_1)\times P_{R_2}(A_2)$, therefore each element of
  $P_{R_1}(A_1)\times P_{R_2}(A_2)$ can be identified at most $6$ times, the
  number of possible values for ``$\op$''. 

  In particular, for a fixed ``$\op$'', an element of $P_{R_1}(A_1)\times
  P_{R_2}(A_2)$ can be identified at most once.  To see this, consider two
  different sets $s_1, s_2 \in P_{J_C}(A)$. Let the pairs $(a_1,a_2)$,
  $(b_1,b_2)$, $a_1,b_1\in R_1$, $a_2,b_2\in R_2$, be such that $s_1 = A \cap
  J^{\op}_{a_1,a_2}$ and $s_2 = A \cap J^{\op}_{b_1,b_2}$. Suppose that $a_1\cap
  A_1 = b_1\cap A_1$  ($\in P_{R_1}(A_1)$) and $ a_2 \cap A_2 = b_2 \cap A_2$
  ($\in P_{R_2}(A_2)$). The set $s_1$ can be seen as $\{(t_1,t_2) ~:~ t_1\in
  a_1\cap A_1, t_2\in a_2\cap A_2 \mbox{ s.t.  } t_1.C \op t_2.C\}$. Analogously
  the set $s_2$ can be seen as $\{(t_1,t_2) ~:~ t_1\in b_1\cap A_1, t_2\in
  b_2\cap A_2 \mbox{ s.t. } t_1.C \op t_2.C\}$. But given that $a_1\cap A_1 =
  b_1\cap A_1$ and $ a_2 \cap A_2 = b_2 \cap A_2$, this leads to $s_1=s_2$, a
  contradiction. Hence, a pair $(c_1,c_2)$, $c_1\in P_{R_1}(A_1)$, $c_2\in
  P_{R_2}(A_2)$ can only be identified at most $6$ times, one for each possible
  value of ``$\op$''.  

  Thus,$ |P_{J_C} (A)|\leq 6|P_{R_1} (A_1)|\cdot |P_{R_2} (A_2)|.$ $A$ could not
  be shattered if $|P_{J_C}(A)|< 2^v$. Since 
  \[
  |P_{J_C}(A)|\leq 6|P_{R_1} (A_1)|\cdot |P_{R_2} (A_2)|\leq
  6g(v_1,v)g(v_2,v)\leq 6v^{v_1+v_2}, %< 2^v.
  \]
  it is sufficient to have $6v^{v_1+v_2}\le 2^v$, which holds for $v>
  3(v_1+v_2)\log(v_1+v_2)$.
\end{proof}

%The relative complexity of this proof is due to the fact that the result of a
%join between the output of two selection queries obviously depends on the two
%selection predicates, and we can not ignore this. 

%Mapping the member of the Cartesian product $\Tab_1\times\Tab_2$ to points in
%$\mathbb{R}^2$ and then trying to see the queries as halfspaces is not a viable
%option, not only because the selection predicates are therefore ignored but also
%because equi-joins (i.e., join queries where ``$\op$'' is ``$=$'') can not be
%seen as half spaces (or better, as the intersection of two halfspaces) because
%of the possible presence of multiple tuples $t_1,t_2,\dots$ in one of the two
%tables that share the same value on the join column, i.e. $t_1.C=t_2.C=\dots$.
%These tuples are then mapped to points of $\mathbb{R}^2$ that, if part of the
%join output, will make the output impossible to represent as the
%union/intersection of a known-in-advance number of half spaces.

The above results can be generalized to any query plan represented as a tree
where the select operations are in the leaves and all internal nodes are join
operations. As we said earlier, such a tree exists for any query.

\begin{lemma}\label{lem:vcdimmuljoin}
  Consider the class $Q$ of queries that can be seen as combinations of select
  and joins on $u>2$ tables $\Tab_1,\dots,\Tab_u$. Let $S_i=(\Tab_i,R_i)$,
  $i=1,\dots,u$ be the range space associated with the select queries on the $u$
  tables. Let $v_i=\VC(S_i)$. Let $m$ be the maximum number of columns in a table
  $\Tab_i$. We assume $m\le \sum_i v_i$.\footnote{The assumption $m\le \sum_i
  v_i$ is reasonable for any practical case.} Let $S_Q = (\Tab_1\times\dots\times
  \Tab_u, R_Q)$ be the range space associated with the class $Q$. The range set
  $R_Q$ is defined as follows. Let $\rho = (r_1,\dots,r_u)$, $r_i\in R_i$, and
  let $\omega$ be a sequence of
  $u-1$ join conditions representing a possible way to join the $u$ tables $\Tab_i$,
  using the operators $\{>,<,\ge,\le,=,\neq\}$. We define the range 
  \[
  J^\omega_{\rho} = \{(t_1,\dots,t_u) ~:~ t_i\in r_i, \mbox{ s.t. }
  (t_1,\dots,t_u) \mbox{ satisfies } \omega\}.\]
  $R_Q$ is the set of all possible $J^\omega_{\rho}$. Then,
  \[
  \VC(S_Q)\leq 4u(\sum_i \VC(S_i))\log(u\sum_i \VC(S_i)).
  \]
\end{lemma}

Note that this Lemma is not just an extension of Lemma~\ref{lem:vcdimjoin}
to join queries between two multicolumns tables. Instead, it is an extension to
queries containing joins between multiple tables (possibly between multicolumns
tables).

\begin{proof}
Assume that a set $A\subseteq T_J$ is shattered by $R_Q$, and $|A|=v$. Consider
the cross-sections $A_i=\{x\in\Tab_i ~:~
(y_1,\dots,y_{i-1},x,y_{i+1},\dots,y_u)\in A\}, 1\le i\le u$.  Note that
$|A_i|\leq v$ and by~\citep[Coroll.~14.4.3]{AlonS08} %~\ref{corol:SauerProj} 
$|P_{R_i} (A_i)|\leq g(v_i,v)
 \le v^{v_i}$. For each set $r \in P_{J_C}(A)$
(i.e., for each subset $r\subseteq A$, given that $P_{J_C}(A)=2^A$)  there is a
sequence $\rho=(r_1,\dots,r_u)$, $r_i\in R_i$, and there is an $\omega$, such
that $r = A \cap J^{\omega}_{\rho}$. Each sequence $\rho$ identifies a distinct
sequence $(r_1\cap A_1, r_2\cap A_2, \dots, r_u\cap A_u) \in
P_{R_1}(A_1)\times\dots\times P_{R_u}(A_u)$, therefore each element of
$P_{R_1}(A_1)\times \dots\times P_{R_u}(A_u)$ can be identified at most
$6^{u-1}$ times, one for each different $\omega$.

In particular, for a fixed $\omega$, an element of $P_{R_1}(A_1)\times
\dots\times P_{R_u}(A_u)$ can be identified at most once.
To see this, consider two different sets $s_1, s_2 \in
P_{J_C}(A)$. Let the vectors $\rho_a=(a_1,\dots,a_u)$,
$\rho_b=(b_1,\dots,b_u)$, $a_i,b_i\in R_i$, be such that $s_1 = A \cap
J^{\omega}_{\rho_a}$ and $s_2 = A \cap
J^{\omega}_{\rho_b}$. Suppose that $a_i\cap A_i = b_i\cap A_i$  ($\in
P_{R_i}(A_i)$). The set $s_1$ can be seen as $\{(t_1,\dots,t_u) ~:~ t_i\in a_i\cap
A_i,\mbox{ s.t. } (t_1,\dots,t_u) \mbox{ satisfies } \omega\}$. Analogously the
set $s_2$ can be seen as $\{(t_1,\dots, t_u) ~:~ t_i\in b_i\cap A_i, \mbox{ s.t.
} (t_1,\dots,t_u) \mbox{ satisfies } \omega\}$. But given that $a_i\cap A_i =
b_i\cap A_i$, this leads to $s_1=s_2$, a contradiction. Hence, a vector 
$(c_1,\dots,c_u)$, $c_i\in P_{R_i}(A_i)$, can only be identified at most a
finite number $\ell$ of times, once for each different $\omega$. For each of the $u-1$ join conditions
composing $\omega$ we need to choose a pair $(\Tab_1.A,\Tab_1.B)$ expressing the
columns along which the tuples should be joined. There are at most $g=\binom{um}{2}$
such pairs (some of them cannot actually be chosen, e.g., those of the type
$(\Tab_1.A, \Tab_1.B)$). There are then $\binom{g}{u-1}$ ways of choosing these
$u-1$ pairs. For each choice of $u-1$ pairs, there are $6^{(u-1)}$ ways of
choosing the operators in the join conditions ($6$ choices for $\op$ for each
pair). We have
\[
\ell\le \binom{\binom{um}{2}}{u-1}\cdot6^{(u-1)}\le (mu)^{2u}.
\]
Thus, $|P_{J_C} (A)|\leq \ell|P_{R_1} (A_1)|\cdot \dots \cdot|P_{R_u} (A_u)|$.
$A$ could not be shattered if $|P_{J_C}(A)|< 2^v$. Since we have
\begin{align*}
|P_{J_C} (A)| &\leq \ell\cdot|P_{R_1} (A_1)|\cdot \dots\cdot |P_{R_u} (A_u)|\leq
\ell\cdot g(v_1,v)g(v_2,v)\dots g(v_u,v) \leq \\
&\leq (mu)^{2u} \cdot v^{v_1+\dots+v_u}, %2^v.
\end{align*}
then it is sufficient to have 
\[
(mu)^{2u}v^{\sum_{i=1}^v vi}\le 2^v,
\]
which holds for $v> 4u\left(\sum_i v_i\right)\log (u\sum_i v_i)$.
\end{proof}

\subsection{General Queries}\label{sec:vcfreqvcdimgenqueries}
Combining the above results we prove:
\begin{theorem}\label{thm:vcfreqvcdimgenqueries}
Consider a class $Q_{u,m,b}$ of all queries with up to $u-1$ join and $u$ select
operations, where each select operation involves no more than $m$ columns and $b$
Boolean operations, then 
\[
\VC(Q_{u,m,b}) \leq
12u^2(m+1)b\log((m+1)b)\log(3u^2(m+1)b\log((m+1)b)).\]
\end{theorem}

Note that Thm.~\ref{thm:vcfreqvcdimgenqueries} gives an upper bound to the
VC-dimension. Our experiments suggest that in most cases the VC-dimension and
the corresponding minimum sample size are even smaller.

\section{Estimating Query Selectivity}\label{sec:vcfreqapplications}
We apply the theoretical result on the VC-dimension of queries 
to constructing a concrete
algorithm for selectivity estimation and query plan optimization.

\subsection{The general scheme}
Our goal is to apply Def.~\ref{def:eapprox} and Thm.~\ref{thm:eapprox} to
compute an estimate of the selectivity of SQL queries. Let $Q_{u,m,b}$ be a
class of queries as in Thm.~\ref{thm:vcfreqvcdimgenqueries}.
%, for some parameters $u$, $m$, and $b$. 
The class $Q_{u,m,b}$ defines a range space $S=(X,R)$ such that $X$ is the
Cartesian product of the tables involved in executing queries in $Q_{u,m,b}$,
and $R$ is the family of all output sets of queries in $Q_{u,m,b}$. 
%Let $\VC(S)$ be the VC-dimension of $S$ and let $\Sam$ be an $\varepsilon$-approximation of
%$X$.
% of size $m$ as defined by Thm.~\ref{thm:eapprox}. 
Let $\Sam$ be an $\varepsilon$-approximation of $X$ and
let $r$ be the output set
of a query $q\in Q_{u,m,b}$ when executed on the original dataset, then $X\cap
r=r$ and $r\cap \Sam$ is the output set when the query is executed on the sample
(see details below). Thus, by Def.~\ref{def:eapprox},
\[
\left|\frac{|X\cap r|}{|X|} - \frac{|\Sam\cap r|}{|\Sam|}\right|=
|\sigma_\Db(q)-\sigma_\Sam(q)| \le\varepsilon,
\]
i.e., the selectivity of running a query $q\in Q_{u,m,b}$ on an $\varepsilon$-approximation of $X$ 
is
within $\varepsilon$ of the selectivity of $q$ when executed on the original set of tables.
Note that for any execution plan of a query $q\in Q_{u,m,b}$, all the queries
that correspond to subtrees rooted at internal nodes of the plan are queries in
$Q_{u,m,b}$. Thus, by running query $q$ on an $\varepsilon$-approximation of $X$ we obtain accurate
estimates for the selectivity of all the subqueries defined by its execution
plan. Corresponding results are obtained by using a relative
  $(p,\varepsilon)$-approximation for $X$.
  
\subsection{Building and using the sample representation}\label{sec:vcfreqbuilding}
We apply Thm.~\ref{thm:eapprox} to probabilistically construct an $\varepsilon$-approximation of $X$.
A technical difficulty in algorithmic application of the theorem is that it is proven  for
a uniform sample of the Cartesian product
of all the tables in the database, while
in practice it is more efficient to maintain the table structure of the
original database in the sample. It is easier to sample each table
independently, and to run the query on a sample that consists of subsets of the
original tables rather than re-writing the query to run on a Cartesian product
of tuples. However, the Cartesian product of independent uniform samples of
tables is not a uniform sample of the Cartesian product of the
tables~\citep{ChaudhuriMN99}. We developed the following procedure to circumvent
this problem. Assume that we need a uniform sample of size $t$ from
$\mathbf{D}$, which is the Cartesian product of $\ell$ tables
$\Tab_1,\dotsc,\Tab_\ell$. We then sample $t$ tuples uniformly at random (with
replacement) from
each table $\Tab_i$, to form a sample table $\Sam_i$. We add an attribute
$sampleindex$ to each $\Sam_i$ and we set the value in the added attribute for each tuple in
$\Sam_i$ to a unique value in $[1,t]$. Now, each sample table will contain $t$ tuples,
each tuple with a different index value in $[1,t]$. Given an index value
$i\in[1,t]$, consider the set of tuples $X_i=\{x_1,\dotsc,x_\ell\}$, $x_j\in\Sam_i$
such that $x_1.sampleindex = x_2.sampleindex =\dotsb=x_\ell.sampleindex=i$. $X_i$
can be seen as a tuple sampled from $\mathbf{D}$, and the set of all $X_i$,
$i\in[1,t]$ is a uniform random sample of size $t$ from $\mathbf{D}$. We run
queries on the sample tables, but in order to estimate the selectivity of a join
operation we count a tuple $Y$ in the result only if the set of tuples composing
$Y$ is a subset of $X_i$ for some $i\in[1,t]$. This is easily done by scanning
the results and checking the values in the $sampleindex$ columns (see Algorithms~\ref{alg:CreateSam} and~\ref{alg:ComputeSel}).
%Lemma~\ref{lem:ComputeSel}, XXX

\begin{algorithm}[ht]
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwFunction{drawRandomTuple}{drawRandomTuple}
\DontPrintSemicolon
\Input{sample size $s$, tables $\Tab_1,...,\Tab_k$.}
\Output{sample tables $\Sam_1,...,\Sam_k$ with $t$ tuples each.}
\For{$j\leftarrow 1$ \KwTo $k$}{
$\Sam_j\leftarrow\emptyset$\;
}
\For{$i\leftarrow 1$ \KwTo $s$}{
\For{$j\leftarrow 1$ \KwTo $k$}{
$t\leftarrow$ \drawRandomTuple{$\Tab_j$}\;
$t.sampleindex_j\leftarrow i$\;
$\Sam_j \leftarrow \Sam_j \cup \{t\}$\;
}
}
\caption{$\mathtt{CreateSample}(s,(T_1,\dots,T_k))$}\label{alg:CreateSam}
\end{algorithm}

\begin{algorithm}[ht]
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwFunction{executeOperation}{executeOperation}
\Input{elementar database operation $op$, sample database $S=(\Sam_1,\dots,\Sam_k)$ of
size $s$.}
\Output{the selectivity of $op$.}
$O_{op} \leftarrow \executeOperation{S, op}$\;
$(\ell_1,\dots,\ell_j)\leftarrow$ indexes of the sample tables involved in $op$ \;
$i\leftarrow 0$\;
\For{ $tuple\in O_{op}$}{
\lIf{$tuple.sampleindex_{\ell_1}=tuple.sampleindex_{\ell_2}=\dots=tuple.sampleindex_{\ell_j}$}{
$i\leftarrow i+1$\;
}
}
$selectivity \leftarrow i/s$\;
\caption{$\mathtt{ComputeSelectivity}(\Sam,op)$}\label{alg:ComputeSel}
\end{algorithm}

\begin{theorem}\label{lem:ComputeSel}
The $\mathtt{ComputeSelectivity}$ procedure (in Alg.~\ref{alg:ComputeSel})
executes a query on the Cartesian product of independent random samples of the
tables but outputs the selectivity that corresponds to executing the query on a
random sample of the Cartesian product of the original tables. 
\end{theorem}

\begin{proof}
The $CreateSample$ procedure chooses from each table a random sample of
$t$ tuples and adds to each sampled tuple an index in $[1,t]$. Each sample table has
exactly one tuple with each index value, and the Cartesian product of the sample
tables has exactly one element that is a concatenation of tuples, all with the
same index $i$ in their tables. Restricting the selectivity computation to these
$t$ elements (as in $ComputeSelectivity$) gives the result. 
\end{proof}

Note that our method circumvent the major difficulty pointed out
by~\citet{ChaudhuriMN99}. They also proved that, in general, it
is impossible to predict sample sizes for given two tables such that the join of
the samples of two tables will result in a sample of a required size out of the
join of the two tables. Our method does not require a sample of a given size
from the result of a join. The VC-dimension sampling technique requires only a
sample of a given size from the Cartesian product of the tables, which is
guaranteed by the above procedure.

Identifying the optimal query plan during query optimization may
require executing several candidate query plans on the sample. A standard
bottom-up candidate plan generation allows us to execute sub-plans once, store
their results and reuse them multiple times as they will be common to many
candidate plans. While the overhead of this execution-based selectivity
estimation approach will still likely be higher than that of pre-computation
based techniques (e.g., histograms),  the reduced execution times of highly
optimized plans enabled by better estimates, especially for complex and
long-running queries, will more than compensate for this overhead.   
Thus, storing intermediate results that are common to several executions will speed up
the total execution time on the sample. The significant improvement in the
selectivity estimates in complex queries well compensates for the extra work in
computing the selectivity estimates.

\section{Experiments}\label{sec:vcfreqexperiments}
This section presents the results of the experiments we run to validate our
theoretical results and to compare our selectivity estimation
method with standard techniques implemented in PostgreSQL and in
Microsoft SQL Server.

\paragraph{Goals.} The first goal of the experiments is to evaluate the practical
usefulness of our theoretical results. To assess this, we run queries on a large
database and on random samples of different sizes. We use the selectivity
of the each query in the random samples as an estimator for the selectivity in
the large database with the adjustments for join operations, as
described in the previous Section. We compute the error between the
estimate and the actual selectivity to show that the thesis of
Thm.~\ref{thm:eapprox} is indeed valid in practice. The use of a large number
of queries and of a variety of parameters allows us to evaluate the error
rate as a function of the sample size. We then compare our
method with the commonly used selectivity estimation based  on precomputed
histograms (briefly described in Sect.~\ref{sec:vcfreqselhist}). We use histograms
with a different number of buckets to show that, no matter how fine-grained the
histograms might be, as soon as the inter-column and intra-bucket assumptions
are no longer satisfied, our approach gives better selectivity
estimates.

\subsection{Selectivity Estimation with Histograms}\label{sec:vcfreqselhist}
In many modern database systems, the query optimizer relies on
histograms for computing data distribution statistics to help determine the most
efficient query plans. In particular, PostgreSQL 
uses one-dimensional equi-depth (i.e., equal frequency buckets) histograms and a
list of the most common values (MCV) for each column (of a database table) to
compute optimizer statistics. The MCV information stores the most
frequent $N$ items (by default $N=100$) and their frequency for each column. The
histograms (by default with $100$ bins) are built for the values not stored in
the MCV list. The selectivity of a constraint $A=x$, where $A$ is a
column and $x$ is a value is computed from the MCV list if $x$ is in the MCV
list or from the histogram bin that contains $x$ if $x$ is not in the MCV list.
The selectivity of a range constraint such as $A<x$ is computed with information from both the MCV
list and the histogram, i.e., the
frequencies of the most common values less than x and the frequency estimate for
$A<x$ from the histogram will be added to obtain the selectivity.

In PostgreSQL, the histograms and the MCV lists for the
columns of a table are built using a random sample of the tuples of the table. The
histograms and the MCV list for all columns of a table are based on the same
sample tuples (and are therefore correlated).  The sample size is computed for
each column using a formula based on the table size, histogram size, and a
target error probability developed by~\citet{ChaudhuriMN98}  and the largest sample size required by
the columns of a table is used to set the sample size of the table. 

Finally, the join selectivity of multiple constraints are computed using
the attribute independence assumption: e.g., selectivities are added in case of an
OR operator and multiplied for an AND operator.  Therefore, large selectivity
estimation errors are possible for complex queries and correlated inputs. 

\subsection{Setup}

\paragraph{Original tables.} The tables in our large database were randomly
generated and contain 20 million tuples each. There is a distinction between tables
used for running selection queries and tables used for running join (and
selection) queries. For tables on which we run selection queries only, the
distributions of values in the columns fall in two different categories:  
\begin{itemize}
  \item {\bf Uniform and Independent:} The values in the columns are chosen
    uniformly and independently at random from a fixed domain (the integer
    interval $[0,200000]$, the same for all columns). Each column is treated
    independently from the others. 
  \item {\bf Correlated:} Two columns of the tables contain values following a
    multivariate normal distribution with mean $M=\mu\mathbb{I}_{2,2}$ and a
    non-identity covariance matrix $\Sigma$ (i.e., the values in the two
    different columns are correlated). 
\end{itemize}
The tables for join queries should be considered in pairs $(A,B)$ (i.e., the
join happens along a common column $C$ of tables $A$ and $B$). The values in the
columns are chosen uniformly and independently at random from a fixed domain (the integer interval
$[0,200000]$, the same for all columns). Each column is treated independently
from the others. 

\paragraph{Sample tables.} We sampled tuples from the large tables uniformly,
independently, and with replacement, to build the sample tables. For the samples
of the tables used to run join queries, we drew random tuples uniformly at
random from the base tables independently and added a column $sampleindex$ to
each tuple such that each tuple drawn from the same base table has a different
value in the additional column and with tuples from different tables forming an
element of the sample (of the Cartesian product of the base tables) if they have
the same value in this additional column, as described in
Sect.~\ref{sec:vcfreqbuilding}.

For each table in the original database we create many sample tables of
different sizes. The sizes are either fixed arbitrarily or computed
using~\eqref{eq:vceapprox} from Thm.~\ref{thm:eapprox}. The arbitrarily sized
sample tables contain between $10000$ and $1.5$ million tuples. To compute the
VC-dimension-dependent sample size, we fixed $\varepsilon=0.05$,
$\delta=0.05$, and $c=0.5$. The parameter $d$ was set to the best bound to the
VC-dimension of the range space of the queries we were running, as obtained from
our theoretical results. If we let $m$ be the number of columns involved in the
selection predicate of the queries and $b$ be the number of Boolean clauses in
the predicate, we have that $d$ depends directly on $m$ and $b$, as does the
sample size $s$ through \eqref{eq:vceapprox} in Thm.~\ref{thm:eapprox}. For
selection queries, we used $m=1,2$ and $b=1,2,3,5,8$, with the addition of the
combination $m=5$, $b=5$. We run experiments on join queries only for some
combinations of $m$ and $b$ (i.e., for $m=1$ and $b=1,2,5,8$) due to the large
size of the resulting sample tables. Table~\ref{tab:samplesize} shows the sample
sizes, as number of tuples, for the combinations of parameters we used in our
experiments.

\begin{table}[htb]
  \centering
  %\tbl{Sample sizes}{
  \begin{tabular}{cccccc}
    %\cline{3-6}
    \toprule
    & & \multicolumn{4}{c}{Query type} \\
    \cmidrule(l){3-6}
    & &
    \multicolumn{2}{c}{Select} &
    \multicolumn{2}{c}{Join} \\
    \cmidrule(l){3-4} \cmidrule(l){5-6} 
    Columns ($m$) & Boolean clauses ($b$) & VC-dim & Sample size & VC-dim & Sample size \\
    \midrule
    \multirow{5}{*}{1} & 1 & 2 & 1000 & 4 & 1400\\
     & 2 & 4 & 1400 & 16 & 3800\\
     & 3 & 6 & 2800 & 36 & 7800 \\
     & 5 & 10 & 2600 & 100 & 20600\\
     & 8 & 16 & 3800 & 256 & 51800\\
     \midrule
    \multirow{4}{*}{2} & 2 & 31 & 6800  & & \\
     & 3 & 57 & 12000 & & \\
     & 5 & 117 & 24000 & & \\
     & 8 & 220 & 44600 & & \\
     \midrule
    5 & 5 & 294 & 59400 & & \\
    \bottomrule
  \end{tabular}
  %}
  \caption{VC-Dimension bounds and samples sizes, as number of tuples, for the
  combinations of parameters we used in our experiments. }
  \label{tab:samplesize}
\end{table}
 
%Nevertheless, although the sample sizes may appear very large both relatively to
%the original tables and in absolute terms, it is important to remember that the
%sample size is independent from the sizes of the original tables, so the larger
%the original table, the smaller will be the ratio between the sample size and
%the original size and the higher the gains in terms of space. Also, for join
%operations, the sample size should be compared to the size of the Cartesian
%product of the table involved, i.e. with the product of the sizes, making the
%sample size actually very small in comparison. This is true also for the sample
%size of selection queries: no matter whether our tables are part of a data
%warehousing environment or they are just slightly larger than the available
%central memory, the needed sample size is the same, but the relative gain in
%terms of space is very different and our approach becomes more and more
%practical as the sizes of the original tables increase.

\paragraph{Histograms.} We built histograms with a different number of buckets,
ranging from $100$ to $10000$. Due to limitations in PostgreSQL, incrementing
the number of buckets in the histograms also increments the number of values
stored in the MCV list. Even if this fact should have a positive influence on
the quality of the selectivity estimates obtained from the histograms, our
results show that the impact is minimal, especially when the inter-column
independence and the intra-bucket uniformity assumptions are not satisfied.
For SQL Server, we built the standard single-column histograms and
computed the multi-column statistics which should help obtaining better
estimations when the values along the columns are correlated.

\paragraph{Queries.} 
For each combination of the parameters $m$ and $b$ and each large table (or pair
of large tables, in the case of join) we created $100$ queries, with selection
predicates involving $m$ columns and $b$ Boolean clauses. The parameters in each
clause, the range quantifiers, and the Boolean operators connecting the
different clauses were chosen uniformly at random to ensure a wide coverage of
possible queries.

\subsection{Results}
\paragraph{Selection Queries.} The first result of our experiments is that, for
all the queries we ran, on all the sample tables, the estimate of the
selectivity computed using our method was within
$\varepsilon$ ($=0.05$) from the real selectivity. The same was not true for the
selectivity computed by the histograms. As an example, in the case of $m=2$,
$b=5$ and uniform independent values in the columns, the default PostgreSQL
histograms predicted a selectivity more than $\varepsilon$ off from the real
selectivity for 30 out of 100 queries.  Nevertheless, in some of cases the
histograms predicted a selectivity closer to the actual one than what our method
predicted. This is especially true when the histogram independence assumption
holds (e.g., for $m=2$, $b=5$ the default histograms gave a better prediction
than our technique in 11 out of 100 cases). Similar situations also arise for
SQLServer.

\begin{figure}[tp]
  \centering
  \includegraphics[scale=0.30]{vcfreq/T_k5_u200k_unif_k2_b5_errperc}
  %\vspace{-0.3cm}
  \caption{Selectivity prediction error for selection queries on a table with
  uniform independent columns -- Two columns ($m=2$), five Boolean clauses ($b=5$).}
  \label{fig:vcfreqT_k5_u200k_unif_k2_b5_errperc}
\end{figure}

Since the selectivity estimated by the our method was always within $\varepsilon$
from the actual, we report the actual percent error, i.e., the quantity
$e_\%=\frac{100|p(\sigma_q)-\sigma_\Db(q)|}{\sigma_\Db(q)}$ where $p(\sigma_q)$
is the predicted selectivity. We analyze the average and the standard deviation
of this quantity on a set of queries and the evolution of these measures as
the sample size increases. We can see from
Fig.~\ref{fig:vcfreqT_k5_u200k_unif_k2_b5_errperc}
%~\ref{fig:vcfreqT_k5_u200k_unif_k5_b5_errperc},
and~\ref{fig:vcfreqT_k2_correl_k2_b8_errperc} that both the average and the standard
deviation of the percentage error of the prediction obtained with our method
decrease as the sample size grows (the rightmost plotted
sample size is the one from Table~\ref{tab:samplesize}, i.e., the one computed 
in~Thm.\ref{thm:eapprox}. More interesting is the comparison in those figures between the performance of the
histograms and the performance of our techniques in predicting selectivities. When
the assumptions of the histograms hold, as is the case for the data plotted in
%Figures~\ref{fig:vcfreqT_k5_u200k_unif_k5_b5_errperc} and
Fig.~\ref{fig:vcfreqT_k5_u200k_unif_k2_b5_errperc}, the predictions obtained from the
histograms are good. 

\begin{figure}[tp]
  \centering
  \includegraphics[scale=0.3]{vcfreq/T_k2_correl_k2_b8_errperc}
  %\vspace{-0.3cm}
  \caption{Selectivity prediction error for selection queries on a table with
  correlated columns -- Two columns ($m=2$), eight Boolean clauses ($b=8$).}
  \label{fig:vcfreqT_k2_correl_k2_b8_errperc}
\end{figure}

But as soon as the data are
correlated (Fig.~\ref{fig:vcfreqT_k2_correl_k2_b8_errperc}), our sampling method gives better
predictions than the histograms even at the smallest sample sizes and keeps
improving as the sample grows larger. It is also interesting to observe how the
standard deviation of the prediction error is much smaller for our method than
for the histograms, suggesting a much higher consistency in the quality of the
predictions. In Fig.~\ref{fig:vcfreqT_k2_correl_k2_b8_errperc} we do not show multiple
curves for the different PostgreSQL histograms because increasing the number of
buckets had very marginal impact on the quality of the estimates, sometimes even
in the negative sense (i.e., an histogram with more buckets gave worse
predictions than an histogram with fewer buckets), a fact that can be explained
with the variance introduced by the sampling process used to create the
histograms. For the same reason we do not plot multiple lines for the
prediction obtained from the multi-columns and single-column statistics of SQL
Server: even when the multi-column statistics were supposed to help, as in the
case of correlated data, the obtained prediction were not much different from
the ones obtained from the single-column histograms.

\paragraph{Join Queries.} The strength of our method compared to histograms is
even more evident when we run join queries, even when the histograms independent assumptions are
satisfied. In our experiments, the predictions obtained using our technique were
always within $\varepsilon$ from the real values, even at the smallest sample
sizes, but the same was not true for histograms. For example, in the case of
$m=1$ and $b=5$, $135$ out of $300$ predictions from the histograms were more
than $\varepsilon$ off from the real selectivities.
Figure~\ref{fig:vcfreqjoin_k1_b1_errperc} shows the comparison between the average and
the standard deviation of the percentage error, defined in the previous
paragraph, for the histograms and our method. The numbers include predictions
for the selection operations at the leaves of the query tree.

\begin{figure}[tp]
  \centering
  \includegraphics[scale=0.3]{vcfreq/join_k1_b1_errperc}
  %\vspace{-0.3cm}
  \caption{Selectivity prediction error for join queries queries. One column
  ($m=1$), one Boolean clause ($b=1$).}
  \label{fig:vcfreqjoin_k1_b1_errperc}
\end{figure}

Again,
we did not plot multiple curves for histograms with a different number of
buckets because the quality of the predictions did not improve as the histograms
became more fine-grained. To understand the big discrepancy between the accurate
predictions of our method and the wrong estimates computed by the histograms in
PostgreSQL  we note that  for some join queries, the
histograms predicted an output size on the order of the hundreds of thousands of
tuples but the actual output size was zero or a very small number of tuples.
Observing the curves of
the average and the standard deviation of the percentage error for the
prediction obtained with our method, we can see that at the smaller sample sizes
the quality of the predictions only improves minimally with the sample size.
This is due to the fact that at small sizes our prediction for the join
operation is very often zero or very close to zero, because the output of the
query does not contain enough pairs of tuples from the sample of the Cartesian
product of the input table (i.e., pairs of tuples with the same value in the
$sampleindex$ column). In these cases, the prediction can not be accurate at all
(i.e., the error is 100\% if the original output contained some tuples, or 0\% if
the query returned an empty set in the large databases). As soon as the sample
size grows more, we can see first a jump to higher values of the percentage
error, which then behaves as expected, i.e., decreasing as the sample size
increases. 

In Fig.~\ref{fig:vcfreqjoin_k1_b1_errperc} we also show a
comparison between the percentage error of predictions obtained using our method
in two different ways: the ``theoretically correct'' way that makes use of the
number of pairs of tuples with the same value in the $sampleindex$ column and
the ``practitioner'' way which uses the size of the output of the join operation
in the sample, therefore ignoring the $sampleindex$ column. Recall that we had
to add the $sampleindex$ column because Thm.~\ref{thm:eapprox} requires a
uniform sample of the Cartesian product of the input tables.
As it is evident from Fig.~\ref{fig:vcfreqjoin_k1_b1_errperc}, the ``practitioner''
way of predicting selectivity gives very good results at small sample sizes
(although it does not offer theoretical guarantees). These results are similar
in spirit, although not equivalent, to the theoretical conclusions presented
by~\citet{HaasNSS96} in the setting of selectivity estimation
using online sampling.

%\paragraph{Comments.} From the experiments we ran we can conclude that our
%method provides practically efficient and highly accurate technique for selectivity estimation. 
%The experiments also demonstrate that histograms, even when fine-grained and
%leveraged by the use of a MCV list, give poor estimates compared to our technique.
%The majoThe impossibility (by construction) to handle correlation in
%data makes them almost completely unsuitable for some real world applications
%requiring accurate selectivity estimates. It is also to be noted that, as the
%complexity of a query increases (in the sense of the number of operations
%involved) the more it is possible that the tuples at higher levels (towards the
%root) of the query plan become correlated, thus invalidating the histogram
%independence assumptions even if the base data are not correlated.  A comparison of the
%predictions obtained using our method and those obtained from a multidimensional
%histograms would be interesting to see how more ingenuous techniques may improve
%the usefulness of precomputed statistics. Nevertheless it is important to notice
%that in order to be able to take into account the possible correlations among
%all pairs of columns in a table, an histogram with an exponential (in the number
%of columns) number of dimension is needed, and it could still give low-quality
%selectivity predictions for queries whose complexity is greater than what the
%histogram could handle. Another interesting fact pointed out by our experiments
%is that a sample of the Cartesian product may not actually be needed, as the
%results in Fig.~\ref{fig:vcfreqjoin_k1_b1_errperc} shows, i.e., the bounds to the
%sample size may be such that a certain level of non-uniformity in the sampling
%process may be accommodated. It may even well be that the entire
%Thm.~\ref{thm:eapprox}, does not need the sample to be a uniform sample of the
%set of points.
%
\section{Conclusions}\label{sec:vcfreqcompar}
We develop a novel method for estimating the selectivity of queries by executing
it on a concise, properly selected, sample of the database. We present a
rigorous analysis of our method and extensive experimental results demonstrating
its efficiency and the accuracy of its predictions.

%As we discussed in Section~\ref{sec:vcfreqprevwork},
Most commercial databases use histograms built on a single column, for selectivity
estimation. There has also been significant research on improving the estimate
using multidimensional
histograms~\citep{BrunoCG01,PoosalaI97,SrivastavaHMKT,WangS03} and join
synopses~\citep{AcharyaGPR99}. The main advantage of our method is that it gives
uniformly accurate estimates for the selectivity of any query within a predefined
VC-dimension range. Methods that collect and store pre-computed statistics give
accurate estimates only for the relations captured by the collected statistics,
while estimate of any other relation relies on an independence assumption.
%, which give probabilistic guarantees on the error of the predicted
%selectivity,%
%Flexibility is one of the major aspects making our approach more suitable for
%practical implementation than the above advanced techniques (which, despite
%being presented some years ago, still have to be implemented in widespread
%DBMS). Once the parameters $m$, $b$, and $u$ for the class of queries to be run
%on the database have been fixed, the sample can be created and used to estimate
%the selectivities of the queries. From the way we defined our class of queries
%it should be clear that such a sample is able to give accurate estimation no
%matter what tables are joined together and no matter what the join columns in
%those table is. At the same time, the selectivity of any possible selection
%query in the class can be correctly estimated, independently of what columns
%appear in the selection predicate and how the different clauses composing the
%predicate are connected with Boolean operators. This make our approach extremely
%flexible.  

To match the accuracy of our new method with histograms and join synopses
one would need to create, for each table, a multidimensional histogram where the
number of dimensions is equal to the number of columns in the tables. The space
needed for a multidimensional histogram is exponential in the number of
dimensions, while the size of our sample representation is almost linear in that parameter. 
Furthermore, to estimate the selectivity for join operations
one would need to create join synopses for all pairs of columns in the database,
again in space that grows exponential in the number of columns.

It is interesting to note that the highly theoretical concept of VC-dimension
leads in this work to an efficient and practical tool for an important data
analysis problem.

%In the future, we will study the possibility of using results from the
%VC-dimension theory to solve the problem of computing good approximations of the
%results of aggregate functions applied to the outputs of database queries.

\section*{Acknowledgements}
This work was supported in part by the National Science Foundation, under grant
IIS-0905553.

\vskip 0.2in
%\bibliography{vcfreq,vcmine,riondapubs}

\iffalse
\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acharya et~al.(1999)Acharya, Gibbons, Poosala, and
  Ramaswamy]{AcharyaGPR99}
Swarup Acharya, Phillip~B. Gibbons, Viswanath Poosala, and Sridhar Ramaswamy.
\newblock Join synopses for approximate query answering.
\newblock \emph{SIGMOD Rec.}, 28:\penalty0 275--286, June 1999.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/304181.304207}.
%\newblock URL \url{http://doi.acm.org/10.1145/304181.304207}.

\bibitem[Alon and Spencer(2008)]{AlonS08}
Noga Alon and Joel~H. Spencer.
\newblock \emph{The Probabilistic Method}.
\newblock Interscience Series in Discrete Mathematics and Optimization. John
  Wiley {\&} Sons, Hoboken, NJ, USA, third edition, 2008.

\bibitem[Anthony and Bartlett(1999)]{AnthonyB99}
Martin Anthony and Peter~L. Bartlett.
\newblock \emph{Neural Network Learning - Theoretical Foundations}.
\newblock Cambridge University Press, New York, NY, USA, 1999.
%\newblock ISBN 978-0-521-57353-5.

\bibitem[Babcock et~al.(2003)Babcock, Chaudhuri, and Das]{BabcockCD03}
Brian Babcock, Surajit Chaudhuri, and Gautam Das.
\newblock Dynamic sample selection for approximate query processing.
\newblock In \emph{Proceedings of the 2003 ACM SIGMOD international conference
  on Management of data}, SIGMOD '03, pages 539--550, New York, NY, USA, 2003.
  ACM.
%\newblock ISBN 1-58113-634-X.
%\newblock \doi{http://doi.acm.org/10.1145/872757.872822}.
%\newblock URL \url{http://doi.acm.org/10.1145/872757.872822}.

\bibitem[Benedikt and Libkin(2002)]{BenediktL02}
Michael Benedikt and Leonid Libkin.
\newblock Aggregate operators in constraint query languages.
\newblock \emph{Journal of Computer and System Sciences}, 64\penalty0
  (3):\penalty0 628--654, 2002.
%\newblock ISSN 0022-0000.
%\newblock \doi{DOI: 10.1006/jcss.2001.1810}.
%\newblock URL
%  \url{http://www.sciencedirect.com/science/article/pii/S0022000001918100}.

\bibitem[Blum et~al.(2008)Blum, Ligett, and Roth]{BlumLR08}
Avrim Blum, Katrina Ligett, and Aaron Roth.
\newblock A learning theory approach to non-interactive database privacy.
\newblock In \emph{Proceedings of the 40th annual ACM symposium on Theory of
  computing}, STOC '08, pages 609--618, New York, NY, USA, 2008. ACM.
%\newblock ISBN 978-1-60558-047-0.
%\newblock \doi{http://doi.acm.org/10.1145/1374376.1374464}.
%\newblock URL \url{http://doi.acm.org/10.1145/1374376.1374464}.

\bibitem[Blumer et~al.(1989)Blumer, Ehrenfeucht, Haussler, and
  Warmuth]{BlumerEHW89}
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred~K. Warmuth.
\newblock Learnability and the {V}apnik-{C}hervonenkis dimension.
\newblock \emph{Journal of the ACM}, 36:\penalty0 929--965, October 1989.
%\newblock ISSN 0004-5411.
%\newblock \doi{http://doi.acm.org/10.1145/76359.76371}.
%\newblock URL \url{http://doi.acm.org/10.1145/76359.76371}.

\bibitem[Brown and Haas(2006)]{BrownH06}
Paul~G. Brown and Peter~J. Haas.
\newblock Techniques for warehousing of sample data.
\newblock In \emph{Proceedings of the 22nd International Conference on Data
  Engineering}, ICDE '06, page~6, Washington, DC, USA, 2006. IEEE Computer
  Society.
%\newblock ISBN 0-7695-2570-9.
%\newblock \doi{http://dx.doi.org/10.1109/ICDE.2006.157}.
%\newblock URL \url{http://dx.doi.org/10.1109/ICDE.2006.157}.

\bibitem[Bruno and Chaudhuri(2004)]{BrunoC04}
Nicolas Bruno and Surajit Chaudhuri.
\newblock Conditional selectivity for statistics on query expressions.
\newblock In \emph{Proceedings of the 2004 ACM SIGMOD international conference
  on Management of data}, SIGMOD '04, pages 311--322, New York, NY, USA, 2004.
  ACM.
%\newblock ISBN 1-58113-859-8.
%\newblock \doi{http://doi.acm.org/10.1145/1007568.1007604}.
%\newblock URL \url{http://doi.acm.org/10.1145/1007568.1007604}.

\bibitem[Bruno et~al.(2001)Bruno, Chaudhuri, and Gravano]{BrunoCG01}
Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano.
\newblock {STH}oles: a multidimensional workload-aware histogram.
\newblock \emph{SIGMOD Rec.}, 30:\penalty0 211--222, May 2001.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/376284.375686}.
%\newblock URL \url{http://doi.acm.org/10.1145/376284.375686}.

\bibitem[Chaudhuri et~al.(1998)Chaudhuri, Motwani, and
  Narasayya]{ChaudhuriMN98}
Surajit Chaudhuri, Rajeev Motwani, and Vivek Narasayya.
\newblock Random sampling for histogram construction: how much is enough?
\newblock \emph{SIGMOD Rec.}, 27:\penalty0 436--447, June 1998.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/276305.276343}.
%\newblock URL \url{http://doi.acm.org/10.1145/276305.276343}.

\bibitem[Chaudhuri et~al.(1999)Chaudhuri, Motwani, and
  Narasayya]{ChaudhuriMN99}
Surajit Chaudhuri, Rajeev Motwani, and Vivek Narasayya.
\newblock On random sampling over joins.
\newblock In \emph{Proceedings of the 1999 ACM SIGMOD international conference
  on Management of data}, SIGMOD '99, pages 263--274, New York, NY, USA, 1999.
  ACM.
%\newblock ISBN 1-58113-084-8.
%\newblock \doi{http://doi.acm.org/10.1145/304182.304206}.
%\newblock URL \url{http://doi.acm.org/10.1145/304182.304206}.

\bibitem[Chaudhuri et~al.(2007)Chaudhuri, Das, and Narasayya]{ChaudhuriDN07}
Surajit Chaudhuri, Gautam Das, and Vivek Narasayya.
\newblock Optimized stratified sampling for approximate query processing.
\newblock \emph{ACM Trans. Database Syst.}, 32:\penalty0 50, June 2007.
%\newblock ISSN 0362-5915.
%\newblock \doi{http://doi.acm.org/10.1145/1242524.1242526}.
%\newblock URL \url{http://doi.acm.org/10.1145/1242524.1242526}.

\bibitem[Chazelle(2000)]{Chazelle00}
Bernard Chazelle.
\newblock \emph{The discrepancy method: randomness and complexity}.
\newblock Cambridge University Press, New York, NY, USA, 2000.
%\newblock ISBN 0-521-00357-1.

\bibitem[Chen et~al.(1990)Chen, McNamee, and Matloff]{ChenMM90}
Meng~Chang Chen, Lawrence McNamee, and Norman~S. Matloff.
\newblock Selectivity estimation using homogeneity measurement.
\newblock In \emph{Proceedings of the Sixth International Conference on Data
  Engineering}, pages 304--310, Washington, DC, USA, 1990. IEEE Computer
  Society.
%\newblock ISBN 0-8186-2025-0.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645475.654001}.

\bibitem[Das(2009)]{Das09}
Gautam Das.
\newblock Sampling methods in approximate query answering systems.
\newblock In John Wang, editor, \emph{Encyclopedia of Data Warehousing and
  Mining}, pages 1702--1707. IGI Global, Hershey, PA, USA, 2nd edition, 2009.

\bibitem[Dobra(2005)]{Dobra05}
Alin Dobra.
\newblock Histograms revisited: when are histograms the best approximation
  method for aggregates over joins?
\newblock In \emph{Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART
  symposium on Principles of database systems}, PODS '05, pages 228--237, New
  York, NY, USA, 2005. ACM.
%\newblock ISBN 1-59593-062-0.
%\newblock \doi{http://doi.acm.org/10.1145/1065167.1065196}.
%\newblock URL \url{http://doi.acm.org/10.1145/1065167.1065196}.

\bibitem[Estan and Naughton(2006)]{EstanN06}
Cristian Estan and Jeffrey~F. Naughton.
\newblock End-biased samples for join cardinality estimation.
\newblock In \emph{Proceedings of the 22nd International Conference on Data
  Engineering}, ICDE '06, pages 20--, Washington, DC, USA, 2006. IEEE Computer
  Society.
%\newblock ISBN 0-7695-2570-9.
%\newblock \doi{http://dx.doi.org/10.1109/ICDE.2006.61}.
%\newblock URL \url{http://dx.doi.org/10.1109/ICDE.2006.61}.

\bibitem[Ganguly et~al.(1996)Ganguly, Gibbons, Matias, and
  Silberschatz]{GangulyGMS96}
Sumit Ganguly, Phillip~B. Gibbons, Yossi Matias, and Avi Silberschatz.
\newblock Bifocal sampling for skew-resistant join size estimation.
\newblock \emph{SIGMOD Rec.}, 25:\penalty0 271--281, June 1996.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/235968.233340}.
%\newblock URL \url{http://doi.acm.org/10.1145/235968.233340}.

\bibitem[Ganti et~al.(2000)Ganti, Lee, and Ramakrishnan]{GantiLR00}
Venkatesh Ganti, Mong-Li Lee, and Raghu Ramakrishnan.
\newblock {ICICLES}: Self-tuning samples for approximate query answering.
\newblock In \emph{Proceedings of the 26th International Conference on Very
  Large Data Bases}, VLDB '00, pages 176--187, San Francisco, CA, USA, 2000.
  Morgan Kaufmann Publishers Inc.
%\newblock ISBN 1-55860-715-3.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645926.672017}.

\bibitem[Garcia-Molina et~al.(2002)Garcia-Molina, Ullman, and
  Widom]{GarciaMolinaUW02}
Hector Garcia-Molina, Jeffrey~D. Ullman, and Jennifer Widom.
\newblock \emph{Database Systems - The Complete Book}.
\newblock Prentice Hall, Upper Saddle River, NJ, USA, 2002.
%\newblock ISBN 978-0-13-187325-4.

\bibitem[Gemulla et~al.(2006)Gemulla, Lehner, and Haas]{GemullaLH06}
Rainer Gemulla, Wolfgang Lehner, and Peter~J. Haas.
\newblock A dip in the reservoir: maintaining sample synopses of evolving
  datasets.
\newblock In \emph{Proceedings of the 32nd international conference on Very
  large data bases}, VLDB '06, pages 595--606, Almaden, CA, USA, 2006. VLDB
  Endowment.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=1182635.1164179}.

\bibitem[Gemulla et~al.(2007)Gemulla, Lehner, and Haas]{GemullaLH07}
Rainer Gemulla, Wolfgang Lehner, and Peter~J. Haas.
\newblock Maintaining bernoulli samples over evolving multisets.
\newblock In \emph{Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART
  symposium on Principles of database systems}, PODS '07, pages 93--102, New
  York, NY, USA, 2007. ACM.
%\newblock ISBN 978-1-59593-685-1.
%\newblock \doi{http://doi.acm.org/10.1145/1265530.1265544}.
%\newblock URL \url{http://doi.acm.org/10.1145/1265530.1265544}.

\bibitem[Getoor et~al.(2001)Getoor, Taskar, and Koller]{GetoorTK01}
Lise Getoor, Benjamin Taskar, and Daphne Koller.
\newblock Selectivity estimation using probabilistic models.
\newblock \emph{SIGMOD Rec.}, 30:\penalty0 461--472, May 2001.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/376284.375727}.
%\newblock URL \url{http://doi.acm.org/10.1145/376284.375727}.

\bibitem[Gibbons and Matias(1998)]{GibbonsM98}
Phillip~B. Gibbons and Yossi Matias.
\newblock New sampling-based summary statistics for improving approximate query
  answers.
\newblock \emph{SIGMOD Rec.}, 27:\penalty0 331--342, June 1998.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/276305.276334}.
%\newblock URL \url{http://doi.acm.org/10.1145/276305.276334}.

\bibitem[Gross-Amblard(2011)]{Gross11}
David Gross-Amblard.
\newblock Query-preserving watermarking of relational databases and xml
  documents.
\newblock \emph{ACM Trans. Database Syst.}, 36:\penalty0 3:1--3:24, March 2011.
%\newblock ISSN 0362-5915.
%\newblock \doi{http://doi.acm.org/10.1145/1929934.1929937}.
%\newblock URL \url{http://doi.acm.org/10.1145/1929934.1929937}.

\bibitem[Gryz and Liang(2004)]{GryzL04}
Jarek Gryz and Dongming Liang.
\newblock Query selectivity estimation via data mining.
\newblock In Mieczyslaw~A. Klopotek, Slawomir~T. Wierzchon, and Krzysztof
  Trojanowski, editors, \emph{Intelligent Information Processing and Web
  Mining, Proceedings of the International IIS: IIPWM'04 Conference held in
  Zakopane, Poland, May 17-20, 2004}, Advances in Soft Computing, pages 29--38,
  Berlin Heidelberg, Germany, 2004. Springer-Verlag.

\bibitem[Haas(1996)]{Haas96}
Peter~J. Haas.
\newblock Hoeffding inequalities for join-selectivity estimation and online
  aggregation.
\newblock Technical Report RJ 10040, IBM Almaden Research, 1996.

\bibitem[Haas and K\"{o}nig(2004)]{HaasK04}
Peter~J. Haas and Christian K\"{o}nig.
\newblock A bi-level Bernoulli scheme for database sampling.
\newblock In \emph{Proceedings of the 2004 ACM SIGMOD international conference
  on Management of data}, SIGMOD '04, pages 275--286, New York, NY, USA, 2004.
  ACM.
%\newblock ISBN 1-58113-859-8.
%\newblock \doi{http://doi.acm.org/10.1145/1007568.1007601}.
%\newblock URL \url{http://doi.acm.org/10.1145/1007568.1007601}.

\bibitem[Haas and Swami(1992)]{HaasS92}
Peter~J. Haas and Arun~N. Swami.
\newblock Sequential sampling procedures for query size estimation.
\newblock In \emph{Proceedings of the 1992 ACM SIGMOD international conference
  on Management of data}, SIGMOD '92, pages 341--350, New York, NY, USA, 1992.
  ACM.
%\newblock ISBN 0-89791-521-6.
%\newblock \doi{http://doi.acm.org/10.1145/130283.130335}.
%\newblock URL \url{http://doi.acm.org/10.1145/130283.130335}.

\bibitem[Haas and Swami(1995)]{HaasS95}
Peter~J. Haas and Arun~N. Swami.
\newblock Sampling-based selectivity estimation for joins using augmented
  frequent value statistics.
\newblock In \emph{Proceedings of the Eleventh International Conference on Data
  Engineering}, ICDE '95, pages 522--531, Washington, DC, USA, 1995. IEEE
  Computer Society.
%\newblock ISBN 0-8186-6910-1.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645480.655440}.
%
\bibitem[Haas et~al.(1993)Haas, Naughton, Seshadri, and Swami]{HaasNSS93}
Peter~J. Haas, Jeffrey~F. Naughton, S.~Seshadri, and Arun~N. Swami.
\newblock Fixed-precision estimation of join selectivity.
\newblock In \emph{Proceedings of the twelfth ACM SIGACT-SIGMOD-SIGART
  symposium on Principles of database systems}, PODS '93, pages 190--201, New
  York, NY, USA, 1993. ACM.
%\newblock ISBN 0-89791-593-3.
%\newblock \doi{http://doi.acm.org/10.1145/153850.153875}.
%\newblock URL \url{http://doi.acm.org/10.1145/153850.153875}.

\bibitem[Haas et~al.(1994)Haas, Naughton, and Swami]{HaasNS94}
Peter~J. Haas, Jeffrey~F. Naughton, and Arun~N. Swami.
\newblock On the relative cost of sampling for join selectivity estimation.
\newblock In \emph{Proceedings of the thirteenth ACM SIGACT-SIGMOD-SIGART
  symposium on Principles of database systems}, PODS '94, pages 14--24, New
  York, NY, USA, 1994. ACM.
%\newblock ISBN 0-89791-642-5.
%\newblock \doi{http://doi.acm.org/10.1145/182591.182594}.
%\newblock URL \url{http://doi.acm.org/10.1145/182591.182594}.

\bibitem[Haas et~al.(1996)Haas, Naughton, Seshadri, and Swami]{HaasNSS96}
Peter~J. Haas, Jeffrey~F. Naughton, S.~Seshadri, and Arun~N. Swami.
\newblock Selectivity and cost estimation for joins based on random sampling.
\newblock \emph{J. Comput. Syst. Sci.}, 52:\penalty0 550--569, June 1996.
%\newblock ISSN 0022-0000.
%\newblock \doi{http://dx.doi.org/10.1006/jcss.1996.0041}.
%\newblock URL \url{http://dx.doi.org/10.1006/jcss.1996.0041}.

\bibitem[Har-Peled and Sharir(2011)]{HarPS11}
Sariel Har-Peled and Micha Sharir.
\newblock Relative $(p,\varepsilon)$-approximations in geometry.
\newblock \emph{Discrete \& Computational Geometry}, 45\penalty0 (3):\penalty0
  462--496, 2011.
%\newblock ISSN 0179-5376.
%\newblock URL \url{http://dx.doi.org/10.1007/s00454-010-9248-1}.

\bibitem[Harangsri et~al.(1997)Harangsri, Shepherd, and Ngu]{HarangsriNS97}
Banchong Harangsri, John Shepherd, and Anne H.~H. Ngu.
\newblock Query size estimation using machine learning.
\newblock In \emph{Proceedings of the Fifth International Conference on
  Database Systems for Advanced Applications (DASFAA)}, pages 97--106,
  Hackensack, NJ, USA, 1997. World Scientific Press.
%\newblock ISBN 981-02-3107-5.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=646711.703171}.

\bibitem[Haussler and Welzl(1986)]{HausslerW86}
D~Haussler and E~Welzl.
\newblock Epsilon-nets and simplex range queries.
\newblock In \emph{Proceedings of the second annual symposium on Computational
  geometry}, SCG '86, pages 61--71, New York, NY, USA, 1986. ACM.
%\newblock ISBN 0-89791-194-6.
%\newblock \doi{http://doi.acm.org/10.1145/10515.10522}.
%\newblock URL \url{http://doi.acm.org/10.1145/10515.10522}.

\bibitem[Hou et~al.(1988)Hou, Ozsoyoglu, and Taneja]{HouOT88}
Wen-Chi Hou, Gultekin Ozsoyoglu, and Baldeo~K. Taneja.
\newblock Statistical estimators for relational algebra expressions.
\newblock In \emph{Proceedings of the seventh ACM SIGACT-SIGMOD-SIGART
  symposium on Principles of database systems}, PODS '88, pages 276--287, New
  York, NY, USA, 1988. ACM.
%\newblock ISBN 0-89791-263-2.
%\newblock \doi{http://doi.acm.org/10.1145/308386.308455}.
%\newblock URL \url{http://doi.acm.org/10.1145/308386.308455}.

\bibitem[Hou et~al.(1991)Hou, Ozsoyoglu, and Dogdu]{HouOD91}
Wen-Chi Hou, Gultekin Ozsoyoglu, and Erdogan Dogdu.
\newblock Error-constrained count query evaluation in relational databases.
\newblock \emph{SIGMOD Rec.}, 20:\penalty0 278--287, April 1991.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/119995.115837}.
%\newblock URL \url{http://doi.acm.org/10.1145/119995.115837}.

\bibitem[Ioannidis and Poosala(1995)]{IoannidisP95}
Yannis~E. Ioannidis and Viswanath Poosala.
\newblock Balancing histogram optimality and practicality for query result size
  estimation.
\newblock In \emph{Proceedings of the 1995 ACM SIGMOD international conference
  on Management of data}, SIGMOD '95, pages 233--244, New York, NY, USA, 1995.
  ACM.
%\newblock ISBN 0-89791-731-6.
%\newblock \doi{http://doi.acm.org/10.1145/223784.223841}.
%\newblock URL \url{http://doi.acm.org/10.1145/223784.223841}.

\bibitem[Jagadish et~al.(1998)Jagadish, Koudas, Muthukrishnan, Poosala, Sevcik,
  and Suel]{JagadishKMPSS98}
H.~V. Jagadish, Nick Koudas, S.~Muthukrishnan, Viswanath Poosala, Kenneth~C.
  Sevcik, and Torsten Suel.
\newblock Optimal histograms with quality guarantees.
\newblock In \emph{Proceedings of the 24rd International Conference on Very
  Large Data Bases}, VLDB '98, pages 275--286, San Francisco, CA, USA, 1998.
  Morgan Kaufmann Publishers Inc.
%\newblock ISBN 1-55860-566-5.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645924.671191}.

\bibitem[Jermaine et~al.(2004)Jermaine, Pol, and Arumugam]{JermainePA04}
Christopher Jermaine, Abhijit Pol, and Subramanian Arumugam.
\newblock Online maintenance of very large random samples.
\newblock In \emph{Proceedings of the 2004 ACM SIGMOD international conference
  on Management of data}, SIGMOD '04, pages 299--310, New York, NY, USA, 2004.
  ACM.
%\newblock ISBN 1-58113-859-8.
%\newblock \doi{http://doi.acm.org/10.1145/1007568.1007603}.
%\newblock URL \url{http://doi.acm.org/10.1145/1007568.1007603}.

\bibitem[Jin et~al.(2006)Jin, Glimcher, Jermaine, and Agrawal]{JinGJA06}
Ruoming Jin, Leo Glimcher, Chris Jermaine, and Gagan Agrawal.
\newblock New sampling-based estimators for olap queries.
\newblock In \emph{Proceedings of the 22nd International Conference on Data
  Engineering}, ICDE '06, pages 18--, Washington, DC, USA, 2006. IEEE Computer
  Society.
%\newblock ISBN 0-7695-2570-9.
%\newblock \doi{http://dx.doi.org/10.1109/ICDE.2006.106}.
%\newblock URL \url{http://dx.doi.org/10.1109/ICDE.2006.106}.

\bibitem[Joshi and Jermaine(2008)]{JoshiJ08}
Shantanu Joshi and Christopher Jermaine.
\newblock Robust stratified sampling plans for low selectivity queries.
\newblock In \emph{Proceedings of the 2008 IEEE 24th International Conference
  on Data Engineering}, pages 199--208, Washington, DC, USA, 2008. IEEE
  Computer Society.
%\newblock ISBN 978-1-4244-1836-7.
%\newblock \doi{10.1109/ICDE.2008.4497428}.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=1546682.1547159}.

\bibitem[Kaushik et~al.(2005)Kaushik, Naughton, Ramakrishnan, and
  Chakravarthy]{KaushikNRC05}
Raghav Kaushik, Jeffrey~F. Naughton, Raghu Ramakrishnan, and Venkatesan~T.
  Chakravarthy.
\newblock Synopses for query optimization: A space-complexity perspective.
\newblock \emph{ACM Trans. Database Syst.}, 30:\penalty0 1102--1127, December
  2005.
%\newblock ISSN 0362-5915.
%\newblock \doi{http://doi.acm.org/10.1145/1114244.1114251}.
%\newblock URL \url{http://doi.acm.org/10.1145/1114244.1114251}.

\bibitem[Larson et~al.(2007)Larson, Lehner, Zhou, and Zabback]{LarsonLZZ07}
Per-Ake Larson, Wolfgang Lehner, Jingren Zhou, and Peter Zabback.
\newblock Cardinality estimation using sample views with quality assurance.
\newblock In \emph{Proceedings of the 2007 ACM SIGMOD international conference
  on Management of data}, SIGMOD '07, pages 175--186, New York, NY, USA, 2007.
  ACM.
%\newblock ISBN 978-1-59593-686-8.
%\newblock \doi{http://doi.acm.org/10.1145/1247480.1247502}.
%\newblock URL \url{http://doi.acm.org/10.1145/1247480.1247502}.

\bibitem[Li et~al.(2001)Li, Long, and Srinivasan]{LiLS01}
Yi~Li, Philip~M. Long, and Aravind Srinivasan.
\newblock Improved bounds on the sample complexity of learning.
\newblock \emph{Journal of Computer and System Sciences}, 62\penalty0
  (3):\penalty0 516--527, 2001.
%\newblock ISSN 0022-0000.
%\newblock \doi{DOI: 10.1006/jcss.2000.1741}.
%\newblock URL
%  \url{http://www.sciencedirect.com/science/article/B6WJ0-45B65MX-H/2/3b0f2dd50a30dcee8b995d701959f15f}.

\bibitem[Lipton and Naughton(1995)]{LiptonN95}
Richard~J. Lipton and Jeffrey~F. Naughton.
\newblock Query size estimation by adaptive sampling.
\newblock \emph{J. Comput. Syst. Sci.}, 51:\penalty0 18--25, August 1995.
%\newblock ISSN 0022-0000.
%\newblock \doi{10.1006/jcss.1995.1050}.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=211046.211053}.

\bibitem[Lipton et~al.(1990)Lipton, Naughton, and Schneider]{LiptonNS90}
Richard~J. Lipton, Jeffrey~F. Naughton, and Donovan~A. Schneider.
\newblock Practical selectivity estimation through adaptive sampling.
\newblock \emph{SIGMOD Rec.}, 19:\penalty0 1--11, May 1990.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/93605.93611}.
%\newblock URL \url{http://doi.acm.org/10.1145/93605.93611}.

\bibitem[L\"{o}ffler and Phillips(2009)]{LofflerP09}
Maarten L\"{o}ffler and Jeff~M. Phillips.
\newblock Shape fitting on point sets with probability distributions.
\newblock In Amos Fiat and Peter Sanders, editors, \emph{Algorithms - ESA
  2009}, volume 5757 of \emph{Lecture Notes in Computer Science}, pages
  313--324. Springer, Berlin / Heidelberg, 2009.
%\newblock URL \url{http://dx.doi.org/10.1007/978-3-642-04128-0_29}.

\bibitem[Markl et~al.(2007)Markl, Haas, Kutsch, Megiddo, Srivastava, and
  Tran]{MarklHKMST07}
V.~Markl, Peter.~J. Haas, M.~Kutsch, N.~Megiddo, U.~Srivastava, and T.~M. Tran.
\newblock Consistent selectivity estimation via maximum entropy.
\newblock \emph{The VLDB Journal}, 16:\penalty0 55--76, January 2007.
%\newblock ISSN 1066-8888.
%\newblock \doi{http://dx.doi.org/10.1007/s00778-006-0030-1}.
%\newblock URL \url{http://dx.doi.org/10.1007/s00778-006-0030-1}.

\bibitem[Matias et~al.(1998)Matias, Vitter, and Wang]{MatiasVW98}
Yossi Matias, Jeffrey~Scott Vitter, and Min Wang.
\newblock Wavelet-based histograms for selectivity estimation.
\newblock In \emph{Proceedings of the 1998 ACM SIGMOD international conference
  on Management of data}, SIGMOD '98, pages 448--459, New York, NY, USA, 1998.
  ACM.
%\newblock ISBN 0-89791-995-5.
%\newblock \doi{http://doi.acm.org/10.1145/276304.276344}.
%\newblock URL \url{http://doi.acm.org/10.1145/276304.276344}.

\bibitem[Matou\v{s}ek(2002)]{Matousek02}
Ji\v{r}\'{i} Matou\v{s}ek.
\newblock \emph{Lectures on Discrete Geometry}.
\newblock Springer-Verlag, Secaucus, NJ, USA, 2002.
%\newblock ISBN 0387953744.

\bibitem[Miller(1981)]{Miller81}
Rupert~G. Miller.
\newblock \emph{Simultaneous Statistical Inference}.
\newblock Springer series in statistics. Springer-Verlag, New York, NY, USA,
  1981.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and Talwalkar]{MohriRT12}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock The MIT Press, 2012.

\bibitem[Ngu et~al.(2004)Ngu, Harangsri, and Shepherd]{NguHS04}
A.~H.~H. Ngu, B.~Harangsri, and J.~Shepherd.
\newblock Query size estimation for joins using systematic sampling.
\newblock \emph{Distrib. Parallel Databases}, 15:\penalty0 237--275, May 2004.
%\newblock ISSN 0926-8782.
%\newblock \doi{10.1023/B:DAPD.0000018573.35050.25}.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=973183.973203}.

\bibitem[Olken(1993)]{Olken93}
Frank Olken.
\newblock \emph{Random Sampling from Databases}.
\newblock PhD thesis, University of California, Berkeley, 1993.

\bibitem[Poosala and Ioannidis(1997)]{PoosalaI97}
Viswanath Poosala and Yannis~E. Ioannidis.
\newblock Selectivity estimation without the attribute value independence
  assumption.
\newblock In \emph{Proceedings of the 23rd International Conference on Very
  Large Data Bases}, VLDB '97, pages 486--495, San Francisco, CA, USA, 1997.
  Morgan Kaufmann Publishers Inc.
%\newblock ISBN 1-55860-470-7.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645923.673638}.

\bibitem[Poosala et~al.(1996)Poosala, Haas, Ioannidis, and
  Shekita]{PoosalaHIS96}
Viswanath Poosala, Peter~J. Haas, Yannis~E. Ioannidis, and Eugene~J. Shekita.
\newblock Improved histograms for selectivity estimation of range predicates.
\newblock In \emph{Proceedings of the 1996 ACM SIGMOD international conference
  on Management of data}, SIGMOD '96, pages 294--305, New York, NY, USA, 1996.
  ACM.
%\newblock ISBN 0-89791-794-4.
%\newblock \doi{http://doi.acm.org/10.1145/233269.233342}.
%\newblock URL \url{http://doi.acm.org/10.1145/233269.233342}.

\bibitem[R\'{e} and Suciu(2010)]{ReS10}
Christopher R\'{e} and Dan Suciu.
\newblock Understanding cardinality estimation using entropy maximization.
\newblock In \emph{Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART
  symposium on Principles of database systems}, PODS '10, pages 53--64, New
  York, NY, USA, 2010. ACM.
%\newblock ISBN 978-1-4503-0033-9.
%\newblock \doi{http://doi.acm.org/10.1145/1807085.1807095}.
%\newblock URL \url{http://doi.acm.org/10.1145/1807085.1807095}.

\bibitem[Riondato and Upfal(2012)]{RiondatoU12}
Matteo Riondato and Eli Upfal.
\newblock Efficient discovery of association rules and frequent itemsets
  through sampling with tight performance guarantees.
\newblock In Peter~A. Flach, Tijl De~Bie, and Nello Cristianini, editors,
  \emph{Machine Learning and Knowledge Discovery in Databases - European
  Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings,
  Part I}, volume 7523 of \emph{Lecture Notes in Computer Science}, pages
  25--41, Berlin / Heidelberg, 2012. Springer.
%\newblock ISBN 978-3-642-33459-7.

\bibitem[Srivastava et~al.(2006)Srivastava, Haas, Markl, Kutsch, and
  Tran]{SrivastavaHMKT}
U.~Srivastava, Peter~J. Haas, V.~Markl, M.~Kutsch, and T.~M. Tran.
\newblock {ISOMER}: Consistent histogram construction using query feedback.
\newblock In \emph{Proceedings of the 22nd International Conference on Data
  Engineering}, ICDE '06, pages 39--, Washington, DC, USA, 2006. IEEE Computer
  Society.
%\newblock ISBN 0-7695-2570-9.
%\newblock \doi{http://dx.doi.org/10.1109/ICDE.2006.84}.
%\newblock URL \url{http://dx.doi.org/10.1109/ICDE.2006.84}.

\bibitem[Vapnik(1999)]{Vapnik99}
Vladimir~N. Vapnik.
\newblock \emph{The Nature of Statistical Learning Theory}.
\newblock Statistics for engineering and information science. Springer-Verlag,
  New York, NY, USA, 1999.
%\newblock ISBN 9780387987804.

\bibitem[Vapnik and Chervonenkis(1971)]{VapnikC71}
Vladimir~N. Vapnik and Alexey~J. Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Theory of Probability and its Applications}, 16\penalty0
  (2):\penalty0 264--280, 1971.
%\newblock \doi{10.1137/1116025}.

\bibitem[Wang and Sevcik(2003)]{WangS03}
Hai Wang and Kenneth~C. Sevcik.
\newblock A multi-dimensional histogram for selectivity estimation and fast
  approximate query answering.
\newblock In \emph{Proceedings of the 2003 conference of the Centre for
  Advanced Studies on Collaborative research}, CASCON '03, pages 328--342,
  Boston, MA, USA, 2003. IBM Press.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=961322.961374}.

\bibitem[Wang et~al.(1997)Wang, Vitter, and Iyer]{WangVI97}
Min Wang, Jeffrey~Scott Vitter, and Balakrishna~R. Iyer.
\newblock Selectivity estimation in the presence of alphanumeric correlations.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Data Engineering}, ICDE '97, pages 169--180, Washington, DC, USA, 1997. IEEE
  Computer Society.
%\newblock ISBN 0-8186-7807-0.
%\newblock URL \url{http://portal.acm.org/citation.cfm?id=645482.653296}.

\bibitem[Wu et~al.(2001)Wu, Agrawal, and El~Abbadi]{WuAE01}
Yi-Leh Wu, Divyakant Agrawal, and Amr El~Abbadi.
\newblock Applying the golden rule of sampling for query estimation.
\newblock \emph{SIGMOD Rec.}, 30:\penalty0 449--460, May 2001.
%\newblock ISSN 0163-5808.
%\newblock \doi{http://doi.acm.org/10.1145/376284.375724}.
%\newblock URL \url{http://doi.acm.org/10.1145/376284.375724}.

\end{thebibliography}
\fi

