\chapter{Conclusions}\label{ch:conclusions}

The major goal of the work that led to this dissertation was to verify whether
VC-dimension could be used to develop practical sample size bounds for important
data analytics problems. Before this work, VC-dimension was considered of mostly
theoretical significance. We showed that it is possible to compute tight upper
bounds to the VC-dimension of relevant problems from different areas of data
analytics: knowledge discovery, graph analysis, and database management. 

The upper bounds are characteristic quantities of the dataset or of the
problem at hands. Not only they can be easy to compute, but they are usually
small in practice. This means that the sample size needed to obtain high quality
approximations of the results for the problem at hand may not need to depend on
the size of the dataset or on the number of patterns or ``questions'' that one
is interested to answer using the data. The sample sizes obtained using our
bounds are small and can fit into main memory, partially solving the issue of
analyzing very large datasets. Indeed, the practicality of our results is
exemplified by the fact that one of our algorithms (PARMA, see
Ch.~\ref{ch:parma}) was adapted and implemented to be used at Yahoo.

Despite the many advantages of using VC-dimension when analyzing sample sizes
for randomized algorithms, one must not consider it a one-size-fits-all method
as its wide applicability may be hindered by the following factors:
\begin{itemize}
  \item{\bf Need for an efficient algorithm to compute bounds to the
    VC-dimension.} Bounding the VC-dimension of a problem is a challenging but
    good bounds can be found. Nevertheless, even if the bound is tight and it is a
    characteristic quantity of the dataset or of the problem at hand, actually
    computing this quantity may be computational expensive. We saw an example of
    this issue in Sect.~\refmissing, when dealing with the computation of the
    vertex-diameter of a directed or weighted graph. In that case, we were
    blessed by the presence of the logarithm in the bound of the VC-dimension,
    which allowed us to use a loose approximation of the vertex diameter, but
    this may not always be the case. Without a fast procedure to compute the
    upper bound to the VC-dimension (and therefore the needed sample size), the
    use of VC-dimension is severely limited.
  \item{\bf Need for an efficient sampling procedure.} A bound to the
    VC-dimension is not sufficient: the second necessary component is a sampling
    procedure to sample points of the domain according to the appropriate
    probability distribution. We described an example of how to develop such a
    sampling procedure in Sect.~\refmissing. It is of crucial importance that
    the sampling procedure is fast, otherwise many advantages of using sampling
    may be lost.
  \item{\bf Need of drawing independent samples.} In order to apply
    Thm.~\ref{thm:vceapprox}, we need \emph{independent} samples from the
    distribution. At times, this may be a limitation. For example we saw in
    Sect.~\refmissing that the algorithm from~\citep{BrandesP08} draws multiple
    shortest paths at each step but they are dependent, as they all originate
    from the same vertex. By doing this, the algorithm collects more
    information, resulting in low variance in the estimation. Recent
    developments extends Thm.~\ref{thm:vceapprox} to the case of\itodo{FIX}
    sampling, therefore opening the possibility that the independence
    requirement may be relaxed~\citemissing.
  \item{\bf Dependency on $\varepsilon$.} If the probability mass of a range $R$ is
    smaller than $\varepsilon$, then a sample of size as suggested by~\eqref{eq:
    } may not contain any point in $R$. This is especially annoying when a lot
    of ranges may have very small probability masses. In this case, a lower
    $\varepsilon$ should be used in order to compute acceptable approximations
    of these probability masses. The obvious drawback in using a lower
    $\varepsilon$ is that it directly corresponds to a larger sample. Given that
    the sample size depends on $1/\varepsilon^2$, the increase in the number of
    samples may be substantial for a small decrease in $\varepsilon$. A larger
    sample means a slower algorithm, and the advantages of using sampling may be
    lost or hindered. Before using Thm.~\ref{thm:vcapprox} is therefore
    necessary to assess whether we are interested in good approximations of very
    small probability masses. If so, sampling may not be the best choice.
\end{itemize}

\section*{Future work} 
In this dissertation we studied the development and analysis of fast and
efficient algorithms for data analytics by taking into consideration different
aspects of the problem and different solutions. For each of these, there are a
number of possible directions for future research.

VC-dimension is one of many concepts from the broad field of statistical
learning theory. Other tools and results include data-dependent sample
complexity bounds, pseudodimension, Rademacher averages, Bayesian uniform
bounds, just to name a few~\citep{BoucheronBL05,AnthonyB99,DevroyeGL96}. We
showed that VC-dimension can be have a practical impact to speed up algorithms
for data analytics, but it would be interesting to pursuit the use of these
other concepts for the creation of new algorithm or to further reduce the number
of samples needed to approximate. 

In this work we focused on ``one-shot'' sampling, i.e., the algorithms create a
single sample with size sufficient to obtain an approximation of desired
quality. \emph{Data-dependent bounds and Rademacher averages} can be
useful to develop \emph{progressive sampling} algorithms that start from a small
sample size and check a stopping condition to understand whether they sampled
enough to obtain a good approximation. The challenge in creating such algorithms
is in the development and analysis of a stopping condition that can detect when
to stop as soon as possible.  

A bound on the VC-dimension allows to estimate the expectation of 0-1 functions
using their empirical averages. \emph{Pseudodimension} instead works for
\emph{real-valued} functions. Since many datasets are real valued or can be seen
as collection of real valued functions (e.g., audio and video files and sensor
signals), investigating the use of pseudodimension to create sampling-based
randomized algorithms for data analytics problems on real data is an interesting
research direction.

One issue that we only partially tackle in Chapter~\ref{ch:realfis} is that of
\emph{statistical validation of the results}. We already explained in
Chapter~\ref{ch:intro} why only recently it started to receive attention in data
analytics\itodo{DO THIS!}. Although it is of primary importance to go beyond
simple multi-hypothesis testing corrections like the Union bound (a.k.a.~the
Bonferroni inequality) and we achieved it in Chapter~\ref{ch:realfis} for the
problem of frequent itemsets, there are other options to control false
positives. One example is controlling the False Discovery
Rate~\citep{BenjiaminiH95} instead of the probability of having a false positive
(which is what we do in Chapter~\ref{ch:realfis} and is known as controlling the
Family-Wide Error Rate). There is huge room for improvement and the use of
VC-dimension to develop statistical tests to avoid the inclusion of false
positives in the results is only one possible directions. 

Research in the field of data analytics seems to have paid only limited
attention to recent developments in statistics and probability theory. We
believe that there may be room for improvements and significant contributions in
exploring the literature of these and other fields and adapt results and notions
that may have been considered only of theoretical interest to develop efficient
algorithm for practical problems involving the analysis of the ever-growing and
ever-changing amount of data available today.

In Chapter~\ref{ch:parma} we showed how it is possible to exploit the
power and scalability of modern computational platforms like MapReduce to create
algorithms for data analytics that can handle huge datasets that could not be
processed by a single machine. Technology is one of the driving forces behind
the development of new algorithms. It is important to leverage on the properties
of the next-generation of computational platforms and of hardware when
creating algorithms, in order to achieve the best performances. One particularly
interesting direction is the use of parallel/distributed platforms for data
streams. Algorithms for data streams have existed for a long time, but they
usually consider a single stream processed by a single machine. Platforms like
Spark or S4\refmissing, where the stream is partitioned across multiple machines
require new algorithms that exploit their power. We implemented a streaming version
of PARMA that was integrated in Yahoo SAMOA. Adapting other MapReduce algorithms
to a distributed streaming setting or creating new algorithms for such platforms
is a challenging research direction.

