\chapter{Introduction}\label{ch:intro}

\noindent {\bf Thesis statement:} Analyzing very large datasets is an expensive
computational task. We show that it is possible to obtain high-quality
approximations of the results of many analytics tasks by processing only a small
random sample of the data. We use the Vapnik-Chervonenkis (VC) dimension to
derive the sample size needed to guarantee that the approximation is
sufficiently accurate with high probability. This results in very fast
algorithms for mining frequent itemsets and association rules, computing the
betweenness centrality of vertices in a graph, and estimating the selectivity of
database queries.

\section{Motivations}
Advancements in retrieval and storage technologies led to the collection of
data about many aspects of natural and artificial phenomena.
Analyzing these datasets to find useful information is a daunting task that
requires multiple iterations of data cleaning, modeling, and processing. The
cost of analysis can often be split into two parts. One component of the cost is
intrinsic to the complexity of extracting the desired information (e.g.,
dependent on the number of patterns to find, or the number of iterations
needed). The second component depends on the size of the dataset to be examined.
Smart algorithms can help reducing the first component, while the second seems
to be ever increasing as more and more data become available. Indeed the latter
may reduce any improvement of the intrinsic cost as the dataset grows. Since
many datasets are too large to fit into the main memory of a single machine,
analyzing such datasets would require frequent access to disk or even to the
network, resulting in excessively long processing times.

The use of random sampling is a natural solution to the issue we just described:
by analyzing only a small random sample that fits into main memory there is no
need to access the disk or the network. This comes at the inevitable price of
only being able to extract an approximation of the results, but the trade-off
between the size of the sample and the quality of approximation can be studied
using techniques from the theory of probability. 

The intuition behind the use of random sampling also allow us to look at a
different important issue in data analytics: the statistical validation of the
results. Algorithms for analytics often make the implicit assumption that the
available dataset constitutes the entire reality and it contains, in some sense,
a perfect representation of the phenomena under study. This is indeed not
usually the case: not only datasets contain errors due to inherently stochastic
collection procedures, but they also naturally contain only a limited amount of
information about the phenomena. Indeed a dataset should be seen as a random
sample from an unknown data generation process, whose study corresponds to the
study of the phenomena. By looking at the dataset this way, it becomes clear
that the results of analyzing the dataset are approximated. This leads to the
need of validating the results to discriminate between real and spurious
information, which was found to be interesting in the dataset only due to the
stochastic nature of the data generation process. While the need for statistical
validations has been since long acknowledged in life and physical sciences
and it is a central matter of study in statistics, computer science researchers
have only recently started to pay attention to it and to develop methods to 
either validate the results of previously available algorithms or to avoid the
inclusion of spurious information in the results.

The data analytics tasks we deal with are the following:
\begin{itemize}
  \item Frequent itemsets and association rules mining is one of the core tasks
    of data analytics, requiring to find sets of items in a transactional
    dataset that appear in more than a given fraction of the transactions, and
    use this collection to build high-confidence inference rules between sets of
    items.
  \item Betweenness centrality is a measure of the importance of a vertex in a
    graph, corresponding to the fraction of shortest paths going through that
    vertex. 
  \item The selectivity of a database query is the ratio between the size of its
    output and the product of the sizes of the tables in input. Database
    management systems estimate the selectivity of queries to make informed
    scheduling decisions.
\end{itemize}
We choose to focus on important tasks from different areas of data analytics,
rather than a single task or different tasks from the same area. This is
motivated by our goal of showing that random sampling can be used to compute
good approximations for a variety of tasks. Moreover, these tasks share some
common aspects: they involve the analysis of very large datasets and they
involve the estimation of a measure of interest (frequency, confidence,
betweenness centrality, selectivity) for many objects (itemsets, association
rules, vertices, queries). These aspects are common in many other tasks.

As we said, there exists many mathematical tools that can be used to compute a
sample size sufficient to extract a high-quality approximation of the results:
Chernoff/Hoeffding bounds, martingales, tail bound
on polynomials of random variables, and
more~\citep{MitzenmacherU05,AlonS08,DubhashiP09}. We choose to focus on
VC-dimension for the following reasons:
\begin{itemize}
  \item Classical probabilistic tools like the Azuma inequality work by bounding
    the probability that the measure of interest for a single object deviates
    from its expectation by more than a given quantity. One must then apply the
    union bound to get simultaneous guarantees for all the objects. This results
    in a sample size that depends on the logarithm of the number of objects,
    which may be still too much and too loose. Previous works explored the use
    of these tools to solve the problems we are interested in and indeed
    suffered from this drawback. On the other hand, results related to
    VC-dimension give sample sizes that only depend on this combinatorial
    quantity, which can be very small and independent from the number of
    objects.
  \item The use of VC-dimension is not limited to a specific problem or setting.
    The results and techniques related to VC-dimension are widely applicable and
    can be used for many different problems. Moreover, no assumption is made on
    the distribution of the measure of interest, or on the availability of any
    previous knowledge about the data.
\end{itemize}

\section{Overview of contributions}
This dissertation presents sampling-based algorithms for a number of data
analytics tasks: frequent itemsets and association rules mining, betweenness
centrality approximation, and datatabase query selectivity estimation. To
analyze our algorithms we use the Vapnik-Chervonenkis dimension and related
results from the field of sor a long timetatistical learning theory (Ch.~\ref{ch:vcdim}). This
leads to a significant reduction in the sample sizes needed to obtain
high-quality approximations of the results of the mentioned tasks.

\paragraph*{Mining frequent itemsets and association rules.} We show that the
VC-dimension of the problem of mining frequent itemsets and association rules is
tightly bounded by an easy-to-compute characteristic quantity of the dataset.
This allows us to derive the smallest known bound to the sample size needed to
compute very accurate approximations of the collections of patterns
(Ch.~\ref{ch:vcmine}). The combination of the theoretical guarantees offered by
this results with the computational power and scalability of MapReduce allows us
to develop PARMA, a fast distributed algorithm that mines many small samples in
parallel to achieve higher confidence and accuracy. PARMA
is much more scalable and efficient than existing solutions and uses all the
available computational resources (Ch.~\ref{ch:parma}). 

\paragraph*{Statistical validation.} A common issue when mining frequent
itemsets is the inclusion of false positives, i.e., of itemsets that are only
frequent by chance. We develop a method that computes a bound to the VC-dimension
of a collection of itemsets by solving a variant of the Set-Union Knapsack
Problem. This allows us to to find a frequency threshold such that the
collection of frequent itemsets with respect to this threshold will not include
false positives with high probability (Ch.~\ref{ch:realfis}).

\paragraph*{Betweenness centrality.} We show that it is possible to compute very
accurate estimations of the betweenness centrality (and many of its variants) of
all vertices by only performing a limited number of shortest paths computation,
chosen at random. The sample size depends on the VC-dimension of the problem,
which we show to be bounded by the logarithm of the number of vertices in the
longest shortest path (i.e., by the logarithm of the diameter in an unweighted
graph). The bound is tight and interestingly collapses to a small constant when
there is a unique shortest path between every pair of connected vertices. Our
results also allow us to present a tighter analysis of an existing
sampling-based algorithm for betweenness centrality estimation
(Ch.~\ref{ch:centrsampl}).

\paragraph*{Query selectivity estimation.}  We show that the VC-dimension of
classes of database queries is bounded by the complexity of their SQL
expression, i.e., by the maximum number of joins and selection predicates
involved in the queries. We use this bound to show that it is possible to
obtain very accurate estimations of the selectivities by running the queries on
a very small sample of the database. 

%\paragraph*{Outline.} This dissertation is organized as follows.
%
%Chapter~\ref{ch:vcdim} introduces the Vapnik-Chervonenkis dimension and the
%related results we use to develop and analyze the algorithms in later
%chapters.  
%
%Chapter~\ref{ch:vcmine} is concerned with a first algorithm to extract frequent itemsets
%and association rules from a single static random sample of a very large
%transactional dataset.
%%We bound the VC-dimension of the problem and show that it
%%is a characteristic quantity of the dataset. This allows us to compute the
%%sample size sufficient to guarantee a desired level of accuracy and confidence.
%
%Chapter~\ref{ch:parma} expands the results from Ch.~\ref{ch:vcmine} as building
%blocks for a parallel/distributed algorithm that exploits the MapReduce platform
%to achieve higher accuracy and confidence when mining frequent
%itemsets and association rules from random samples. 
%%The algorithm combines
%%sampling, optimization, and parallel/distributed execution to use the available
%%computational resources in an optimal way.
%
%%Chapter~\ref{ch:shatterfi} proposes the development of a progressive sampling
%%algorithm to extract frequent itemsets. The algorithm will start from a small
%%sample and enlarge it as it is needed according to a stopping rule developed
%%using the VC shatter coefficient of the dataset. This is proposed work.
%
%
%Chapter~\ref{ch:realfis} presents the problem of frequent itemsets mining from a
%statistical point of view and introduces a technique to avoid the inclusion
%of false positives in the collection of frequent itemsets.
%
%%Chapter~\ref{ch:graphmine} deals with the extraction of frequent connected
%%subgraphs from huge graphs using random samples. One of the main challenge in
%%this setting is the design of sampling procedure from a uniform
%%distribution among connected subgraphs of a given size. The algorithm we aim to
%%develop will solve this problem by considering the theory behind branching
%%processes and VC-dimension to compute the sufficient sample size. This is
%%proposed work.
%
%Chapter~\ref{ch:centrsampl} deals with computing high-quality approximations of
%the betweenness centrality, a measure of importance, of all vertices in a large
%graph.
%
%Chapter~\ref{ch:vcfreq} presents a bound bound the VC-dimension of SQL
%queries using their syntax and show that a small sample of the database is
%sufficient to accurately estimate the selectivity of queries.
%% We show a
%%connection between statistical learning theory and the class of SQL database
%%queries and develop an algorithm to estimate the selectivity using a very small
%%sample of the original database.
%
%Chapter~\ref{ch:conclusions} contains the conclusions that can be drawn from
%this body of work and some suggestions for future work.


%Analysis of data through the use of inductive reasoning combined with
%deductive mathematical tools, allowed mankind not to sit still in the dark of a
%cave. Whether science actually helped in giving mankind a direction to move
%along, and whether walking down the direction chosen by mankind, actually
%constitutes progress and advancement is open for discussion, but one can argue
%that it is probably better than sitting still in the dark. There is no science
%without data analysis and traditionally scientists have been the only one
%interested in empirical evaluation of models and in their formulation according
%to empirical evidence, that is, to data. In recent years, a paradigm
%shift occurred and many other categories of people recognized the value of data
%collection and analysis: businesses owners, social scientists, medical doctors,
%and even entities traditionally distant from quantitative assessments like
%governments. These groups have taken an interest in data-driven decision making,
%in an effort to to understand and improve the parts of life and society they are
%mostly interested in.
%
%Advancements in technology allow for the collection and storage of larger and
%larger amounts of data regarding almost all aspects of human life and many
%natural phenomena. Collecting and storing data is however only one part of the
%equation to understand the processes regulating human life and nature. The
%other part, known as \emph{analytics}, consists in analyzing the data to
%discover \emph{information} about these processes. In the same way as books are
%nothing but objects made of paper and ink as long as one does not read them,
%find correlations between one book and another, and discover the references and
%the allusions, data are nothing but long sequences of zeroes and ones as long as
%one does not analyze them, find correlations between data and their generating
%process, discover the patterns and the connections. A bibliophile may admire a
%book even just as the result of craftsmanship, but rarely can a scientist
%appreciate a dataset just for the patterns of zeroes and ones it is made of. Data
%is useless until it is analyzed and information is extracted from it, to
%\emph{discover knowledge}.
%
%The huge improvements in the ability to store and retrieve data have
%not always been parallelled by adequate progress in techniques to process it.
%\emph{Big Data} is an umbrella term for a number of issues and problems in the
%management, process, and analysis of data that arise due to the huge
%\emph{volume} of current datasets, their increasing \emph{variety} in terms of
%origin, format, and representation, and the required \emph{velocity} in
%analyzing them. The ``three V's of Big Data'' are the main causes of the
%complexity of modern analytics. New techniques must be introduced to address the
%tree V's because the methods developed in traditional statistics often fall short in
%terms of scalability as function of the data size (volume), applicability to
%diverse data formats like graphs and structured text (variety), and intrinsic
%speed of analysis (velocity).
%
%The subject of investigation of this dissertation is the use of techniques from
%\emph{statistical learning theory}, a modern branch of mathematical statistics,
%to address the three V's of Big Data by developing sampling-based randomized
%algorithms and statistical tests to extract and evaluate collections of
%interesting patterns from very large transactional datasets and huge graphs, and
%accurately perform important tasks in database management. 
%
%Statistical learning theory deals with the problem of making inference and
%drawing conclusions from ``small samples'', a non-asymptotic setting that
%assumes that only a limited amount of data is
%available~\cite{Vapnik98,Vapnik99}. The apparent clash between this ``data
%scarcity'' assumption and the issues arising in Big Data analytics disappears
%when one considers that traditional mathematical statistics techniques have
%mostly been studied ``in the limit'', i.e., assuming an infinite amount of
%available data, while statistical learning theory is concerned with the
%development of methods to analyze finite datasets, even when they are very
%large. The set of tools developed in this field is large but received limited
%application, mostly due to being often considered mostly only of theoretical
%interest. The results of the work presented here show instead that there is a
%large space of applicability of these techniques as they often outperform
%traditional approaches in terms of efficiency and scalability.
%
%Problems of interest examined in this body of works include the extraction
%(\emph{mining}) of frequent itemsets and association rules, the extraction of
%frequent subgraphs, and the computation of the selectivity (size of the output)
%of SQL database queries. These are fundamental tasks in knowledge discovery and
%database management and have been extensively studied in the literature. The
%algorithms we developed offer new insights on these problems through the use of
%statistical learning theory, and at the same time solve them more efficiently
%than previous contributions. The algorithms trade off a small amount of accuracy
%for a large gain in speed by only mining a small random sample of the dataset.
%The main challenge in the development of these algorithms is to understand the
%behavior of the accuracy of the results obtained by mining the sample as
%function of the sample size. We use a tool from statistical learning theory
%known as VC-dimension and results related to it to model this behavior and
%compute the sample size as function of the desired accuracy and confidence, and
%of a characteristic quantity of the dataset that measures the complexity of the
%task at hand. The empirical evaluation shows that the algorithms we developed
%outperform existing methods.
%
%Despite the apparent ``obsession'' with data that seems to be intrinsic in
%analytics, the real goal of knowledge discovery is to and understand the
%\emph{process generating the data}. As the dataset is only but a collection of
%samples from this unknown random process, it only partially reflects the
%process, and can contain noise that can lead to false conclusions about it. It
%is necessary to conduct an assessment of the validity of the results obtained
%during the mining phase by performing a \emph{statistical test of the
%significance of the patterns}. The huge number of potential patterns makes the
%development of such tests a complex matter due to the need of correct for
%\emph{multiple hypotheses} in order to control the probability of misclassifying
%some pattern as significant when it is not. Multiple hypothesis testing has been
%the subject of a large amount of work in the literature. We develop a test that
%use VC-dimension together with optimization techniques
%to find patterns that are frequent according to the unknown distribution of the
%data generating process. We show that the test has high statistical power and
%it is competitive with other existing methods.
%
