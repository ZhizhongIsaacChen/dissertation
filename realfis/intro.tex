\section{Introduction}\label{sec:intro}

The mining of association rules is one of the fundamental primitives in the process
 of knowledge discovery from large data bases. In its most general definition, the problem
 can be reduced to identifying set of items, or \emph{itemsets},
that appear in a fraction at least $\theta$ of all
 transactions, where $\theta$ is provided in input by the user.

One of the main issues in frequent itemsets mining is the presence of spurious
discoveries, or false positives, in the output. These are itemsets that are
reported in output even if their appearance is due only to random associations
in the datasets, and are therefore not \emph{significant}. The presence of false
positives undermines the success of subsequent analyses based on frequent
itemsets.

A number of approaches have been recently proposed to identify
\emph{significant} itemsets, whose appearance in the dataset is not due to
random associations.  These approaches are based on \emph{statistical tests}
that assess the significance of the itemsets by defining a random model that
captures the properties characterizing the association between the items in a
itemset.

For example, consider two items $a$ and $b$ each appearing in $5\%$ of the %100
transactions of a dataset, and assume that we are interested in significant
itemsets that appear in at least $2\%$ of the transactions. In order to avoid to
report spurious discoveries, one can test if the frequency of the itemset $\{a,b\}$ can be
explained entirely by the frequencies of $a$ and $b$, by comparing the
frequency of $\{a,b\}$ with the distribution of the frequency under the
hypothesis that $a$ and $b$ are placed independently into transactions.  For
example, if $\{a,b\}$ appears in $3\%$ of the transactions, they are highly
associated, since the probability that $\{a,b\}$ appears in at least $3\%$ of
the transactions if $a$ and $b$ are independent is only $6\times 10^{-4}$. This
is an example of a statistical test, in which the probability, or $p$-value,
that a measure, or \emph{statistic} (in the example, the frequency of $\{a,b\}$)
is at least as extreme as the value observed in real data is computed under a
\emph{null hypothesis} that captures the properties of spurious discoveries (in
the example, the independence of items). When the $p$-value is small enough, the
itemset is flagged as significant, otherwise the itemset is discarded as a
spurious discovery. A number of different procedures~\citep{SilversteinBM98,MegiddoS98,DuMouchelP01,BoltonHA02,GionisMMT07,Hamalainen10,KirschMAPUV12} have been proposed
in recent years to control the number of spurious discoveries that are reported
\emph{after} the frequent itemsets are identified. These procedures takes into
account the fact that a transactional dataset contains a number of patterns, and
the assessment of their significance therefore is a \emph{multiple
hypothesis testing} problem.

However, there is another source of false discoveries among frequent patterns
that has received scant attention in the literature: the \emph{observed}
transactional dataset is only a finite sample from all possible transactions that may be generated by the 
process resulting in the transactional dataset.
In its most general form, this process can be thought of as a sampling process
from an unknown distribution $p$ over all possible transactions, and in reality
one is not interested in the itemsets that are frequent in the observed dataset,
but in the itemsets that are frequent according to $p$, in the sense that the
probability that they appear in a transaction sampled from $p$ is at least
$\theta$.

For example, consider
the transactions given by items that are bought together on Amazon; after
observing a certain number of transactions, one is interested in inferring
association between items that are valid for the distribution over \emph{all} possible
purchases, not only for the current, observed set $\Ds$ of purchases that
represents only a partial observation obtained from the distribution that
includes also purchases that have not been recorded in the dataset, or that will
materialize only in the future.

The itemsets
that are not frequent in $p$ but are frequent in $\Ds$ are false discoveries, and are not going to be filtered by
the statistical tests described above, since such tests \emph{assume} that the
itemsets that are frequent in $\Ds$, whose significance they assess, represent the frequent itemsets
in $p$ as well. In fact, these tests define itemsets as spurious by considering
properties of the itemsets other than its frequency. In the example above, the
co-occurrence of $\{a,b\}$ is likely not due to random chance; however, the
probability that $\{a,b\}$ appears with frequency $3\%$  in $\Ds$ while its
frequency in $p$ is $1\%$ is $0.08$, therefore if $\theta=2\%$ then $\{a,b\}$ likely is a false
discovery. As noted by~\citet{LiuZW11}, the phase of assessment of the
significance of the frequent itemsets cannot replace the role played by the minimum support
threshold $\theta$, that is to reflect the level of domain significance, and is to be used
in concert with statistical significance to filter uninteresting patterns that
arise from different sources. Therefore being able to rigorously identify frequent patterns in $p$ 
is crucial in order to obtain high quality patterns.

In this paper we address the problem of
identifying itemsets that appear with probability at least $\theta$ in $p$,
that we call \emph{Real Frequent Itemsets} (RFI), while providing rigorous
probabilistic guarantees on the number of false discoveries, without making any
assumption on the particular generative model of the transactions. This makes
our method completely \emph{distribution free}. In particular, we focus on
returning a set of RFI with bounded Family-Wise Error Rate (FWER), that is the
probability that one or more false discovery is reported among the RFI. A
recently proposed alternative to  bound the FWER is to bound the False Discovery
Rate (FDR), that in our case correspond to the proportion of false discoveries
among the RFI. The use of the FDR allows to produce in output a larger number of
patterns, since a small proportion of false discoveries are tolerated in output;
however, in data mining the number of patterns produced is usually high,
therefore having a smaller number of high quality discoveries is preferable to
reporting a larger number of patterns containing some false discoveries.

We stress that we do not assume any generative model on the transactions that
are observed in the transactional dataset. In fact, we only assume that the
transactions in the datasets are independent samples from the distribution $p$,
without any constraint on the properties of $p$. This is in contrast with the
assumptions that are made by the methods that assess the significance after the
frequent patterns have been identified. In fact, these methods require a well
specified, limited model to characterize the significance of a pattern, as it is
in the case of independence in the example above.

\iffalse
{\bf XXX:} we should explain here what a statistical test is, what the FWER,
what the difference with the FDR, and so on. If we do it well here, we probably
do not need to do it again the preliminaries.

{\bf XXX:} We should really stress that we are not mining ``statistically
significant'' itemsets, as this would imply some kind of underlying model (e.g.,
``items appear independently in transactions'') that instead we do not have. I
believe we should actually somewhat comment on such models, which are clearly
too simplistic to be really useful/meaningful: in real/natural data generation
processes itemsets \emph{clearly} do not appear independently, so comparing the
dataset to such a model is only of limited value. 
\fi

\subsection{Our contributions}
We introduce a rigorous statistical test to identify real frequent itemsets that
guarantees that the Family-Wise Error Rate is within the user-specified limits. 
%This is the first
%method that can guarantee a bound to the FWER without using the Bonferroni
%correction. 
The test is based on mathematical tools from statistical learning
theory. We define a range set associated to the problem at hand and give an
upper bound to its (empirical) VC-dimension, showing an interesting connection
with a variant of the knapsack optimization problem.
%, also proving that the bound is tight. XXX
To the best of our knowledge, ours is the first work to apply these
techniques to the field of RFI's, and in general the first application of the sample complexity
bound based on empirical VC-Dimension to the field of data mining. We
implemented our test and evaluated its performances. First of all, we assessed
the values and the behaviour of key parameters of our method, finding them very
reasonable. Secondly, we checked how well the test controls the FWER: we
noticed that it performs in practice even better than what the theory
guarantees. Thirdly, we empirically evaluated
the statistical power of our method, noticing that only a small fraction of the
RFI's is not included in the output collection, i.e., that the test has a high
statistical power. We compared it with other available tests to extract RFI's
and found the power of our method comparable or even better than the current
state of the art. Lastly, we tested whether the frequency in the dataset is a
good estimator for the real frequency of a RFI, answering this questions
positively.

\paragraph*{Outline} The article is organized as follows. In
Section~\ref{sec:prevwork} we review relevant previous contributions.
Section~\ref{sec:prelims} contains preliminaries to formally define the problem
and key concepts that we will use throughout the work. Our statistical test is
described and analyzed in Section~\ref{sec:main}. We present the methodology and
results of our experimental evaluation of the test in
Section~\ref{sec:experiments}. Conclusions and future work are presented in
Section~\ref{sec:concl}.

