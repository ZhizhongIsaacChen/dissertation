\section{Preliminaries}\label{sec:prelims}
In this section we introduce with the right level of formalism all the necessary
definitions, lemmas, and tools that we will use throughout the work.

\subsection{Itemsets mining}\label{sec:itemdef}
Given a ground set $\Itm$ of \emph{items}, let $p$ be a
probability distribution on $2^{\Itm}$. We call a single sample drawn from $p$
a \emph{transaction}. Given a transaction $\tau\in2^{\Itm}$, let $|\tau|$ denote
the number of items in $\tau$. A \emph{dataset} $\Ds$ is a bag of $n$
transactions $\Ds=\{\tau_1,\dots,\tau_n ~:~ \tau_i\subseteq\Itm\}$, i.e., of $n$
\emph{independent identically distributed} (i.i.d.) samples from $p$. Starting
from $p$, we can define another function on the members of $2^{\Itm}$:
\[
r_p(A) = \sum_{\tau\in 2^\Itm,A\subseteq\tau}p(\tau).
\]
Overloading the namespace a little, we say that $r$ is a function on
\emph{itemsets}, which are just members of $2^{\Itm}$. The
difference between itemsets and transactions is that a transaction $\tau$ contains
multiple itemsets $A_1,A_2,\dots,A_{2^{|\tau|}-1}$ (all $\tau$'s non-empty subsets
are itemsets). Given an itemset $A$ we call $r_p(A)$ the \emph{real frequency} of
$A$.

Define $T(A)=\{\tau\in 2^{\Itm} ~:~ A\subseteq \tau\}$. The members of the set
$T(A)$ are all the transactions built on $\Itm$
that contain the itemset $A$. Analogously, given a dataset $\Ds$, let $T_\Ds(A)$ denote
the set of transactions in $\Ds$ that contain $A$. The \emph{frequency} of $A$ is
the fraction of transactions in $\Ds$ that contain $A$: $f_\Ds(A)=
\frac{|T_\Ds(A)|}{|\Ds|}$. It is easy to see that $f_\Ds(A)$ is the
\emph{empirical average} and an \emph{unbiased estimator} for $r_p(A)$. 

In this work, we are interested in finding the itemsets that have
real frequency at least $\theta$ for some $\theta\in(0,1]$. We call the set of
such itemsets the \emph{Real Frequent Itemsets} (RFI's).

%{\bf FV: changed. I think that if we define the problem only based on the distribution, it will not be clear where the problem is.}
\begin{definition}\label{def:mineprobthreshold}
  Given a set of items $\Itm$, a dataset $\Ds$ of i.i.d. samples from a probability distribution $p$ on the transactions
  in $2^\Itm$, and a \emph{minimum real frequency threshold} $\theta$, $0<\theta\le 1$,
  the \emph{RFI's mining task with respect to $\theta$} consists of finding all itemsets
  with real frequency $\geq\theta$, i.e., the set 
  \[ \RFI(p,\Itm,\theta)=\{A ~:~ A \in 2^\Itm \mbox{ and } r_p(A)\ge
  \theta\}.\]
\end{definition}
Note that the definition of RFI's mining task does not depend on the observed dataset $\Ds$, but only on $p,\Itm$, and $\theta$.
It should be clear that, if one is only given a \emph{finite} number of random
samples (the dataset $\Ds$) from $p$ as it is usually the case, one can not aim at finding the exact
set $\RFI(p,\Itm,\theta)$. Indeed, in this work, we will present a method to
extract a collection of itemsets for which we can give probabilistic guarantees
to only include RFI's.

Traditionally, the interest has been on extracting the set of \emph{Frequent
Itemsets} (FI's) from $\Ds$, given a minimum frequency threshold $\theta$.
\begin{definition}\label{def:minefreqthreshold}
  Given a dataset $\Ds$ with transactions built on a ground set $\Itm$, and a
  \emph{minimum frequency threshold} $\theta$, $0<\theta\le 1$, the \emph{FI's
  mining task with respect to $\theta$} consist of finding all itemsets with
  frequency $\geq\theta$, i.e., the set 
  \[ \FI(\Ds,\Itm,\theta)=\{A ~:~ A \in 2^\Itm \mbox{ and } f_{\Ds}(A)\ge
  \theta\}.\]
\end{definition}
Given that $\Ds$ is a set of independent random samples from $p$, no
assumption can be made on the set-inclusion relationship between
$\RFI(p,\Itm,\theta)$ and $\FI(\Ds,\Itm,\theta)$, that is an itemset $A$ in $\RFI(p,\Itm,\theta)$ may 
not appear in $\FI(\Ds,\Itm,\theta)$, and the opposite may happen as well.

While previous work has focused on identifying the FI's, in this work we consider the problem of finding the RFI's instead,
as the final goal of market basket analysis is to gain a better
understanding of the \emph{process generating the data}, i.e., of the real
frequency function $r_p$, which is only partially and noisily captured by the
dataset $\Ds$.

\iffalse
We believe that the collection of RFI's is more interesting than the set of
FI's, as the final goal of market basket analysis is to gain a better
understanding of the \emph{process generating the data}, i.e., of the real
frequency function $r_p$, which is only partially and noisily captured by the
dataset $\Ds$.
\fi

We give here one additional definition that we will use in the analysis of our
method.

\begin{definition}\label{def:negborder}
  Given a collection $S$ of itemsets from $2^{\Itm}$ closed with respect to the
  set inclusion relation, the \emph{negative border} $\mathcal{B}^-(S)$ consists
  of the
  minimal itemsets from $2^{\Itm}$ not in $S$.
\end{definition}
The collection of RFI's and of FI's are always closed with respect to set
inclusion. In these cases, the negative border consists of all the non (real)
frequent itemsets such that all their subsets are (real) frequent itemsets.
\\

\subsection{VC-dimension}\label{sec:prelvcdim}
The Vapnik-Chernovenkis (VC) Dimension of a class of subsets defined
on a set of points is a measure of the complexity or expressiveness of such
class~\citep{VapnikC71}. A finite bound on the VC-dimension of a
structure implies a bound on the number of random samples required to
approximate the expectation of each indicator function associated to a set with
its empirical average. We outline here some basic definitions and results and
refer the reader to the works of~\citet[Sect.~12.4]{DevroyeGL96},
\citet[Sect.~3]{BoucheronBL05}, \citet[Sect.~14.4]{AlonS08}, and
\citet{Vapnik99} for more details on VC-dimension.

Let $X_1^k=(X_1,\dotsc,X_k)$ be a collection of independent identically
distributed random variables taking values in some domain $D$.
For a set $A\subseteq D$, let $\nu(A)=\Pr[X_1\in A]$ and let
\[
\nu_{X_1^k}(A)=\frac{1}{k}\sum_{j=1}^k\mathds{1}_{X_j\in A},\]

where $\mathds{1}_{X_j\in A}$ is the indicator variable for $X_j\in A$.
Let $\mathcal{F}$ be a collection of subsets from $D$. We call $\mathcal{F}$ a
\emph{range set on $D$}.
Given $B\subseteq D$, the \emph{projection of $B$ on $\mathcal{F}$} is the set 
\[
P_\mathcal{F}(X_1^n)=\{ B\cap A; A\in\mathcal{F}\}.\]
The set $B$ is \emph{shattered} by $\mathcal{F}$ if $P_\mathcal{F}(B)=2^B$.

\begin{definition}\label{def:empvcdim}
  Given a set $B\subseteq D$, the \emph{empirical Vapnik-Chervonenkis
  (VC) dimension of $\mathcal{A}$ on $B$}, denoted as $\EVC(\mathcal{F},B)$ is
  the cardinality of the largest subset of $B$ that is shattered by
  $\mathcal{F}$.
\end{definition}

\begin{definition}\label{def:vcdim}
  The \emph{Vapnik-Chervonenkis (VC) dimension of $\mathcal{F}$} is defined as
  $\VC(\mathcal{F})=\EVC(\mathcal{F},D)$.
\end{definition}

The main application of (empirical) VC-dimension in statistics and learning
theory is in computing the size of the sample needed to approximate the $\nu(A)$
with their empirical averages $\nu_{X_1^k}(A)$, for all $A\in\mathcal{A}$.

\begin{definition}\label{def:eapprox}
  Given $\varepsilon\in(0,1)$, a $\varepsilon$-approximation for $\mathcal{F}$
  is a set $S\subseteq D$ such that 
  \[
  \sup_{A\in\mathcal{F}}|\nu(A)-\nu_S(A)|\le\varepsilon.\]
\end{definition}

An $\varepsilon$-approximation can be constructed by random sampling points of
the point space.

\begin{theorem}\label{thm:eapprox}
  \emph{(\citep[Thm.~2.12]{HarPS11}, see also~\citep{LiLS01})}
  Let $\mathcal{F}$ be a range set on $D$ with $\VC(\mathcal{F})\le v$. Given
  $\delta\in(0,1)$ and a positive integer $\ell$, let
  \begin{equation}\label{eq:vceapprox}
    \varepsilon = \sqrt{\frac{c}{\ell}\left(v + \log\frac{1}{\delta}\right)}
  \end{equation}
  where $c$ is an universal positive constant. Then, a set of $\ell$
  i.i.d.~random variables $X_1,\dotsc,X_\ell$ taking values in $D$ is an
  $\varepsilon$-approximation to $\mathcal{A}$ with probability at least
  $1-\delta$.
\end{theorem}
\citet{LofflerP09} showed experimentally that the absolute
constant $c$ is approximately $0.5$.

A similar result holds when an upper bound to the empirical VC-Dimension is
available~\citep{BoucheronBL05}.
\begin{theorem}\label{thm:eapproxempir}
  Let $\mathcal{F}$ be range set on $D$ and
  $X_1^\ell=(X_1,\dotsc,X_\ell)$ be a set of i.i.d. random variables taking
  values in $D$. Let $v$ be an integer such that $\EVC(\mathcal{F},X_1^\ell)\le v$.
   Then, with probability at least $1-\delta$, $X_1^\ell$ is a
  $\varepsilon$-approximation for $X$ for 
  \begin{equation}\label{eq:evceapprox}
    \varepsilon =
    2\sqrt{\frac{2v\log(\ell+1)}{\ell}}+\sqrt{\frac{2\log\frac{2}{\delta}}{\ell}}.
  \end{equation}
\end{theorem}

\subsection{Statistical tests}
\label{sec:stat_tests}

To identify the real frequent itemsets, we use the framework of statistical
hypothesis testing. In statistical hypothesis testing, one is given a \emph{null
hypothesis}, whose rejection, based on a measure, or \emph{statistic}, of the
observed data, corresponds to the identification of a significant phenomenon.

 In our scenario, the naive method to employ statistical hypothesis testing to find the real frequent itemsets is to associate with every itemset $A$ a corresponding null hypothesis $H_A =$ ``$r_p(A) < \theta$'', and compute the $p$-value  for such null hypothesis using  $f_\Ds (A)$ as statistic. In particular, the $p$-value is given by the probability that the frequency of $A$ in a random dataset (with the same number of transactions of $\Ds$) sampled from $p$ is $\ge f_\Ds (A)$ \emph{conditioning} on the event ``$r_p(A)< \theta$''. If the $p$-value is small enough, the null hypothesis is rejected, that is $A$ is flagged as a real significant itemset. When only one hypothesis is considered, the null hypothesis is rejected when the $p$-value is $\le \alpha$, with $\alpha$ usually set to $0.05$ or $0.01$. This guarantees that the  \emph{significance level}, that is the probability of incorrectly reporting an itemset as being in $\RFI(p,\Itm,\theta)$ (type I error), is bounded by $\alpha$.

In our scenario, since we are considering a number of itemsets, we are facing a
\emph{multiple hypothesis testing} problem. In this case, if we use the same
procedure as the single hypothesis case, we could potentially produce a number
of spurious discoveries~\cite{LiuZW11}. In order to avoid this issue, a multiple
hypothesis correction is employed, like the Bonferroni
correction~\cite{DudoitSB03} (see Section~\ref{sec:statpow}).

\begin{definition}\label{def:FWER}
  The Family-Wise Error Rate (FWER) of a statistical test is the probability that at
  least one false positive is reported as significant.
\end{definition}

The Bonferroni correction guarantees that the FWER is bounded by $\alpha$. Sometimes it is useful to relax the guarantees given by the FWER, for examples considering the \emph{False Discovery Rate}.

\begin{definition}\label{def:FDR}
 The False Discovery Rate~\citep{BenjaminiH95} (FDR) is the expected proportion of
 false positives among all itemsets that are reported as significant.
\end{definition}

We focus in this work on identifying a highly reliable collections of high quality itemsets, and therefore aim to identify real frequent itemsets while bounding the FWER of our method, that is we bound the probability that the collection of itemsets reported by our method contains \emph{any} spurious discovery. 

Another important factor for a statistical test is its \emph{power}, that is the probability that the test correctly
rejects the null hypothesis when the null hypothesis is false (also defined as $1-\Pr[\text{type II error}]$).
