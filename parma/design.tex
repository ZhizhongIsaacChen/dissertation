\section{Implementation} 
\label{sec:parmadesign} 
The entire PARMA algorithm has been written as a Java library for
Hadoop, the popular open source implementation of MapReduce. Because all
experiments were done using Amazon Web Service (AWS) Elastic MapReduce,
the version of Hadoop used was 0.20.205, the highest supported by AWS.
The use of Java makes possible future integration with the Apache Mahout
parallel machine learning library (\url{https://mahout.apache.org}). Mahout also
includes an implementation of PFP~\citep{LiWZZC08} that we used for our
evaluation of PARMA.

In PARMA, during the mining phase (i.e. during the reducer of stage 1),
any frequent itemset or association rule mining algorithm can be used.
We wanted to compare the performances of PARMA against PFP which only
produces frequent itemsets, therefore we chose to use a frequent itemset
mining algorithm instead of an association rule mining algorithm. Again,
this choice was merely for ease of comparison with existing parallel
frequent itemset mining algorithms as no such algorithms for association
rule mining exist. While there are many frequent itemset mining
algorithms available, we chose the FP-growth implementation provided
by~\citep{Fp}. We chose FP-growth due to its relative performance
superiority to other Frequent Itemsets mining algorithms. Additionally,
since FP-growth is the algorithm that PFP has parallelized and uses
internally, the choice of FP-growth for the mining phase in PARMA is
appropriate for a more natural comparison.

We also compare PARMA against the naive distributed counting algorithm
(DistCount) for computing frequent itemsets. In this approach, there is
only a single MapReduce iteration. The map breaks a transaction $\tau$
into its powerset $\mathcal{P}(\tau)$ and emits key/value pairs in the
form $(A, 1)$ where $A$ is an itemset in $\mathcal{P}(t)$. The reducers
simply count how many pairs they receive for each itemset $A$ and output
the itemsets with frequency above the minimum frequency threshold. This
is similar to the canonical wordcount example for MapReduce. However,
because the size of the powerset is exponential in the size of the
original transaction (specifically $2^{|\tau|}$, where $|\tau|$ denotes
the number of items in a given transaction), this algorithm incurs
massive network costs, even when combiners are used. This is very similar to the
algorithms presented in~\citep{CryansRC10,LiZ11,YangLF10}. We have built our own
implementation of DistCount in Java using the Hadoop API.
