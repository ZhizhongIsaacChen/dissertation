\section{Previous work}
\label{sec:parmarelated}
We already discussed the relevant previous work about general FI mining and the
use of sampling for this task in Sect.~\ref{sec:vcmineprevwork}. We now focus on the
contributions that are relevant for this chapter: the mining of FIs in a
parallel or distributed setting, the MapReduce framework of computation, and its
use to compute FIs and ARs.

The use of parallel and/or distributed algorithms for Association Rules Mining comes from the
impossibility to handle very large datasets on a single machine. Early
contributions in this area are presented in a survey by~\citet{Zaki99}.
In recent years, the focus shifted to exploit architecture advantages as much as
possible, such as shared memory~\citep{JinYA05}, cluster
architecture~\citep{BuehrerPTKS07} or the massive parallelism of
GPUs~\citep{FangEtAl08}. The main goal is to avoid communication
between nodes as much as possible and minimize the amount of data that are moved
across the 
network~\citep{CongHHP05,EHZaiane06,LiuLZT07,OzkuralUA11}.

The MapReduce~\citep{DeanG08} paradigm enjoys widespread success
across both industry and academia. Research communities in many different
fields uses this novel approach to distributed/parallel computation to develop
algorithms to solve important problems in computer
science~\citep{ChierichettiKT10,ChuKLYBNO06,GoodrichSZ11,LinS10,PietracaprinaPRSU12}.
Not only MapReduce can easily perform computation on very large datasets, but it
is also extremely suited in executing embarassingly parallel algorithms which
make a very limited use of communication. PARMA fits in this description so
MapReduce is an appropriate choice for it.

A number of previous
works~\citep{CryansRC10,GhotingKPK11,Hammoud11,LiWZZC08,LiZ11,YangLF10,ZhouZCLF10}
looked at adapting APriori and FP-growth to the MapReduce setting. Somewhat
naively, some authors~\citep{CryansRC10,LiZ11,YangLF10} suggest a distributed/parallel
counting approach, i.e. to compute the support of every itemset in the dataset
in a single MapReduce round. This algorithm necessarily incurs in a huge data
replication, given that an exponential number of messages are sent to the
reducers, as each transaction contains a number of itemsets that is exponential
in its length. A different adaptation of APriori to MapReduce is presented
in~\cite[Chap.4]{Hammoud11}: similarly to the original formulation of APriori,
at each round $i$, the support for itemsets of length $i$ is computed, and those
that are deemed frequent are then used to generate candidate frequent itemsets
of length $i+1$, although outside of the MapReduce environment. Apart from this,
the major downsides of such approach are that some data replication still
occurs, slowing down the shuffling phase, and that the algorithm does not
complete until the longest frequent itemset is found. Given that length is not
known in advance, the running time of the algorithm can not be
computed in advance. Also the entire dataset needs to be scanned at each round, which
can be very expensive, even if it is possible to keep additional data structures
to speed up this phase.  

An adaptation of FP-Growth to MapRreduce called PFP is presented by~\citet{LiWZZC08}. First,
a parallel/distributed counting approach is used to compute the frequent items,
which are then randomly partitioned into groups. Then, in a single MapReduce
round the transactions in the dataset are used to generate group-dependent
transactions. Each group is assigned to a reducer and the corresponding
group-dependent transactions are sent to this reducer which then builds the
local FP-tree and the conditional FP-trees recursively, computing the frequent
patterns. The group-dependent transactions are such that the local FP-trees and
the conditional FP-trees built by different reducers are independent. This
algorithm suffers from a data replication problem: the number of
group-dependent transactions generated for each single transaction is
potentially equal to the number of groups. This means that the dataset may be
replicated up to a number of times equal to the number of groups, resulting in a
huge amount of data to be sent to the reducers and therefore in a slower
synchronization/communication (\emph{shuffle}) phase, which is usually the most
expensive in a MapReduce algorithm.  Another practical downside of PFP is that
the time needed to mine the dependent FP-tree is not uniform across the groups.
An empirical solution to this load balancing problem is presented
by~\citet{ZhouZCLF10}, although with no guarantees and by computing the groups
outside the MapReduce environment. An implementation of the PFP algorithm as
presented in~\citep{LiWZZC08} is included in Apache Mahout
(\url{http://mahout.apache.org}). \citet{GhotingKPK11} present
an high-level library to perform various machine learning and data mining tasks
using MapReduce. They show how to implement the Frequent Itemset Mining task
using their library. The approach is very similar to that in~\citep{LiWZZC08},
and the same observations apply about the performances and downsides of this
approach.

